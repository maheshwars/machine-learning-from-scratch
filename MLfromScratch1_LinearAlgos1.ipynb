{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57eb0184-f158-43bb-b911-3692fa269ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5775dd2a-6c20-4089-892f-b7be90943a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/maheshwars/Desktop/venv'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62faa6a8-29ed-422e-aec4-660ad0a1a7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        value_min = min(col_values)\n",
    "        value_max = max(col_values)\n",
    "        minmax.append([value_min, value_max])\n",
    "    return minmax\n",
    "    \n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "\n",
    "# calculate column means\n",
    "def column_means(dataset):\n",
    "    means = [0 for i in range(len(dataset[0]))]\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        means[i] = sum(col_values) / float(len(dataset))\n",
    "    return means\n",
    "# calculate column standard deviations\n",
    "def column_stdevs(dataset, means):\n",
    "    stdevs = [0 for i in range(len(dataset[0]))]\n",
    "    for i in range(len(dataset[0])):\n",
    "        variance = [pow(row[i]-means[i], 2) for row in dataset]\n",
    "        stdevs[i] = sum(variance)\n",
    "    stdevs = [sqrt(x/(float(len(dataset)-1))) for x in stdevs]\n",
    "    return stdevs  \n",
    "\n",
    "# standardize dataset\n",
    "def standardize_dataset(dataset, means, stdevs):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i] - means[i]) / stdevs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b19d5bf-685b-4bb1-b499-eb5b778d181e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file /Users/maheshwars/Desktop/venv/data/diabetes.csv with 769 rows and 9 columns\n",
      "[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load dataset\n",
    "filepath = '/Users/maheshwars/Desktop/venv/data'\n",
    "filename = filepath +'/diabetes.csv'\n",
    "dataset = load_csv(filename)\n",
    "print('Loaded data file {0} with {1} rows and {2} columns'.format(filename, len(dataset),\n",
    "len(dataset[0])))\n",
    "\n",
    "# convert string columns to float\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset[1:], i)\n",
    "print(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc3ff22f-6925-4e48-af56-8cbac3fe1324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'], [6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbd8f9e-60d4-46ae-8a1b-dfeaab6e9445",
   "metadata": {},
   "source": [
    "# __Normalisation and Standardisation__\n",
    "Many machine learning algorithms expect the scale of the input and even the output data to be\n",
    "equivalent. It can help in methods that weight inputs in order to make a prediction, such as\n",
    "in linear regression and logistic regression. It is practically required in methods that combine\n",
    "weighted inputs in complex ways such as in artificial neural networks and deep learning.\n",
    "\n",
    "We use **normalization** to refer to rescaling an input variable to the range between 0 and 1. Normalization requires\n",
    "that you know the minimum and maximum values for each attribute.\n",
    "\n",
    "**Standardization** is a rescaling technique that refers to centering the distribution of the data on\n",
    "the value 0 and the standard deviation to the value 1. Together, the mean and the standard\n",
    "deviation can be used to summarize a normal distribution, also called the Gaussian distribution\n",
    "or bell curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d4fc342-86fb-4138-aede-4deef5577b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35294117647058826, 0.7437185929648241, 0.5901639344262295, 0.35353535353535354, 0.0, 0.5007451564828614, 0.23441502988898377, 0.48333333333333334, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Calculate min and max for each column\n",
    "minmax = dataset_minmax(dataset[1:])\n",
    "\n",
    "# Normalize columns\n",
    "normalize_dataset(dataset[1:], minmax)\n",
    "print(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f9fac40-b245-4c0d-8434-ef11ccff34ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35294117647058826, 0.7437185929648241, 0.5901639344262295, 0.35353535353535354, 0.0, 0.5007451564828614, 0.23441502988898377, 0.48333333333333334, 1.0]\n",
      "[0.6395304921176561, 0.8477713205896719, 0.14954329852954315, 0.9066790623472527, -0.6924393247241301, 0.20387990726746852, 0.46818687022979616, 1.4250667195933595, 1.3650063669598014]\n"
     ]
    }
   ],
   "source": [
    "# Estimate mean and standard deviation\n",
    "means = column_means(dataset[1:])\n",
    "stdevs = column_stdevs(dataset[1:], means)\n",
    "\n",
    "print(dataset[1])\n",
    "\n",
    "# standardize dataset\n",
    "standardize_dataset(dataset[1:], means, stdevs)\n",
    "print(dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c4a0b-e306-4d67-a27c-1898e9d86857",
   "metadata": {},
   "source": [
    "## When to Normalize and Standardize\n",
    "\n",
    "Standardization is a scaling technique that assumes your data conforms to a normal distribution.\n",
    "If a given data attribute is normal or close to normal, this is probably the scaling method to use.\n",
    "It is good practice to record the summary statistics used in the standardization process so that\n",
    "you can apply them when standardizing data in the future that you may want to use with your\n",
    "model. Normalization is a scaling technique that does not assume any specific distribution.\n",
    "\n",
    "If your data is not normally distributed, consider normalizing it prior to applying your\n",
    "machine learning algorithm. It is good practice to record the minimum and maximum values\n",
    "for each column used in the normalization process, again, in case you need to normalize new\n",
    "data in the future to be used with your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39702e2-6a85-4824-90d1-24b3c4f9c9d9",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8276122b-955a-402e-80b6-fcd7a1846e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2807b0b-a129-4b91-b289-0e54a64e9047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset into a train and test set\n",
    "def train_test_split(dataset, split=0.60):\n",
    "    train = list()\n",
    "    train_size = split * len(dataset)\n",
    "    dataset_copy = list(dataset)\n",
    "    while len(train) < train_size:\n",
    "        index = randrange(len(dataset_copy))\n",
    "        train.append(dataset_copy.pop(index))\n",
    "    return train, dataset_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd486d5f-f6c2-48b1-b1c4-f711d1066587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3], [11], [2], [7], [1], [8], [9]]\n",
      "[[4], [5], [6], [10]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test train/test split\n",
    "seed(1)\n",
    "dataset = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10],[11]]\n",
    "train, test = train_test_split(dataset)\n",
    "print(train)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a982d0d6-0e39-4d69-8c1b-d0bb034328af",
   "metadata": {},
   "source": [
    "# k-fold Cross-Validation Split\n",
    "\n",
    "A limitation of using the train and test split method is that you get a noisy estimate of\n",
    "algorithm performance. The k-fold cross-validation method (also called just cross-validation) is\n",
    "a resampling method that provides a more accurate estimate of algorithm performance.\n",
    "\n",
    "You should choose a value for k that splits the data into groups with enough rows that each\n",
    "group is still representative of the original dataset.\n",
    "\n",
    "A quick way to check if the fold sizes are representative is\n",
    "to calculate summary statistics such as mean and standard deviation and see how much the\n",
    "values diﬀer from the same statistics on the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d755e173-1724-481d-90f8-d555c8a9a6a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6209378d-7194-4d80-9de4-1766ed429888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset into $k$ folds\n",
    "def cross_validation_split(dataset, folds=3):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / folds)\n",
    "    for i in range(folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61bca0c8-3fcb-44ff-99a6-6b7125c24ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[3], [2]], [[7], [1]], [[8], [9]], [[10], [6]], [[5], [4]]]\n"
     ]
    }
   ],
   "source": [
    "# test cross validation split\n",
    "seed(1)\n",
    "dataset = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]\n",
    "folds = cross_validation_split(dataset, 5)\n",
    "print(folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c200b12-3f3e-4611-b90d-12c10f1c9fc0",
   "metadata": {},
   "source": [
    "* The downside of\n",
    "cross-validation is that it can be time-consuming to run, requiring k diﬀerent models to be\n",
    "trained and evaluated. This is a problem if you have a very large dataset or if you are evaluating\n",
    "a model that takes a long time to train.\n",
    "\n",
    "* Large datasets are those in the hundreds of thousands or millions of records, large enough\n",
    "that splitting it in half results in two datasets that have nearly equivalent statistical properties.\n",
    "In such cases, there may be little need to use k-fold cross-validation as an evaluation of the\n",
    "algorithm and a train and test split may be just as reliable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c0d5b5-1cbf-4a94-a144-a47d64e76214",
   "metadata": {},
   "source": [
    "# Evaluation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b418b6f-96ef-4cdc-bafa-e410c39902be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.0\n"
     ]
    }
   ],
   "source": [
    "# Example of calculating classification accuracy\n",
    "\n",
    "# Calculate accuracy percentage between two lists\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "    # Test accuracy\n",
    "    \n",
    "actual = [0,0,0,0,0,1,1,1,1,1]\n",
    "predicted = [0,1,0,0,0,1,0,1,1,1]\n",
    "accuracy = accuracy_metric(actual, predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aff31334-c8ac-4da1-8ef4-ba4cac25752c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2}\n",
      "[[3, 1, 0], [2, 1, 1], [0, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# calculate a confusion matrix\n",
    "def confusion_matrix(actual, predicted):\n",
    "    unique = set(actual)\n",
    "    matrix = [list() for x in range(len(unique))]\n",
    "    for i in range(len(unique)):\n",
    "        matrix[i] = [0 for x in range(len(unique))]\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for i in range(len(actual)):\n",
    "        x = lookup[actual[i]]\n",
    "        y = lookup[predicted[i]]\n",
    "        matrix[y][x] += 1\n",
    "    return unique, matrix\n",
    "\n",
    "# Test confusion matrix with integers\n",
    "actual = [0,0,0,0,0,2,1,1,1,2]\n",
    "predicted = [0,1,1,0,0,1,0,1,2,2]\n",
    "unique, matrix = confusion_matrix(actual, predicted)\n",
    "print(unique)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ab5221b-0316-450f-8d51-894870d2cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean absolute error (MAE)\n",
    "def mae_metric(actual, predicted):\n",
    "    sum_error = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        sum_error += abs(predicted[i] - actual[i])\n",
    "    return sum_error / float(len(actual))\n",
    "\n",
    "    \n",
    "# Calculate root mean squared error (RMSE)\n",
    "def rmse_metric(actual, predicted):\n",
    "    sum_error = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        prediction_error = predicted[i] - actual[i]\n",
    "        sum_error += (prediction_error ** 2)\n",
    "    mean_error = sum_error / float(len(actual))\n",
    "    return sqrt(mean_error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8916fe96-cec9-4cf2-9545-a2785d3d665b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00894427190999915\n"
     ]
    }
   ],
   "source": [
    "# Test RMSE\n",
    "actual = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "predicted = [0.11, 0.19, 0.29, 0.41, 0.5]\n",
    "rmse = rmse_metric(actual, predicted)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c238f0de-0b7c-43f4-b934-eecb807e1756",
   "metadata": {},
   "source": [
    "RMSE values are always slightly higher than MSE values, which becomes more pronounced as\n",
    "the prediction errors increase. This is a benefit of using RMSE over MSE in that it penalizes\n",
    "larger errors with worse scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6370a989-7c8f-4ef8-9c72-c941b72e8e8b",
   "metadata": {},
   "source": [
    "# __Baseline models__\n",
    "\n",
    "A baseline prediction algorithm provides a set\n",
    "of predictions that you can evaluate as you would any predictions for your problem, such as\n",
    "classification accuracy or RMSE.\n",
    "\n",
    "The scores from these algorithms provide the required point of comparison when evaluating\n",
    "all other machine learning algorithms on your problem. Once established, you can comment on\n",
    "how much better a given algorithm is as compared to the naive baseline algorithm, providing\n",
    "context on just how good a given method actually is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8190d70c-e973-4d3a-beef-33a576323e14",
   "metadata": {},
   "source": [
    "### Random Prediction Algorithm\n",
    "\n",
    "The random prediction algorithm predicts a random outcome as observed in the training data.\n",
    "It is perhaps the simplest algorithm to implement. It requires that you store all of the distinct\n",
    "outcome values in the training data, which could be large on regression problems with lots of\n",
    "distinct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d58df545-8d2d-4080-a8e5-6fa2b79f8a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Example of Making Random Predictions\n",
    "from random import seed\n",
    "from random import randrange\n",
    "\n",
    "# Generate random predictions\n",
    "def random_algorithm(train, test):\n",
    "    output_values = [row[-1] for row in train]\n",
    "    unique = list(set(output_values))\n",
    "    predicted = list()\n",
    "    for _ in test:\n",
    "        index = randrange(len(unique))\n",
    "        predicted.append(unique[index])\n",
    "    return predicted\n",
    "    \n",
    "seed(1)\n",
    "train = [[0], [1], [0], [1], [0], [1]]\n",
    "test = [[None], [None], [None], [None]]\n",
    "predictions = random_algorithm(train, test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8dd37c-cc38-4e78-8bde-c4635ebbe2c5",
   "metadata": {},
   "source": [
    "### Zero Rule Algorithm\n",
    "\n",
    "The Zero Rule Algorithm is a better baseline than the random algorithm. It uses more\n",
    "information about a given problem to create one rule in order to make predictions. This rule is\n",
    "diﬀerent depending on the problem type.\n",
    "\n",
    "one rule is to predict the class value that is most common in\n",
    "the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b02f2e95-c971-47b0-96b6-b6e463e26d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '0', '0', '0']\n",
      "[15.0, 15.0, 15.0, 15.0]\n"
     ]
    }
   ],
   "source": [
    "# Example of Zero Rule Classification Predictions\n",
    "from random import seed\n",
    "\n",
    "# zero rule algorithm for classification\n",
    "def zero_rule_algorithm_classification(train, test):\n",
    "    output_values = [row[-1] for row in train]\n",
    "    prediction = max(set(output_values), key=output_values.count)\n",
    "    predicted = [prediction for i in range(len(test))]\n",
    "    return predicted\n",
    "    \n",
    "seed(1)\n",
    "train = [['0'], ['0'], ['0'], ['0'], ['1'], ['1']]\n",
    "test = [[None], [None], [None], [None]]\n",
    "predictions = zero_rule_algorithm_classification(train, test)\n",
    "print(predictions)\n",
    "\n",
    "# Example of Zero Rule Regression Predictions\n",
    "from random import seed\n",
    "\n",
    "# zero rule algorithm for regression\n",
    "def zero_rule_algorithm_regression(train, test):\n",
    "    output_values = [row[-1] for row in train]\n",
    "    prediction = sum(output_values) / float(len(output_values))\n",
    "    predicted = [prediction for i in range(len(test))]\n",
    "    return predicted\n",
    "    \n",
    "seed(1)\n",
    "train = [[10], [15], [12], [15], [18], [20]]\n",
    "test = [[None], [None], [None], [None]]\n",
    "predictions = zero_rule_algorithm_regression(train, test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af4cf4c-0464-4449-a2d8-943aad5905a4",
   "metadata": {},
   "source": [
    "# Algorithm Test Harness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e8ce0-13de-4dd4-b73d-0d65b9f86fa5",
   "metadata": {},
   "source": [
    "We need a function that can take a dataset and an algorithm and return a performance score.\n",
    "Below is a function named evaluate algorithm() that achieves this. It takes 3 fixed arguments\n",
    "including the dataset, the algorithm function and the split percentage for the train-test split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13e67bd-73cb-4e15-8a3c-34c18b2ed46b",
   "metadata": {},
   "source": [
    "### train-test split harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4bd2782d-b384-4972-b55e-0b18a3fe9384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.427%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate an algorithm using a train/test split\n",
    "def evaluate_algorithm(dataset, algorithm, split, *args):\n",
    "    train, test = train_test_split(dataset, split)\n",
    "    test_set = list()\n",
    "    for row in test:\n",
    "        row_copy = list(row)\n",
    "        row_copy[-1] = None\n",
    "        test_set.append(row_copy)\n",
    "    predicted = algorithm(train, test_set, *args)\n",
    "    actual = [row[-1] for row in test]\n",
    "    accuracy = accuracy_metric(actual, predicted)\n",
    "    return accuracy\n",
    "\n",
    "# Used zero rule algorithm for classification\n",
    "# Test the train/test harness\n",
    "\n",
    "seed(1)\n",
    "\n",
    "# load and prepare data\n",
    "filepath = '/Users/maheshwars/Desktop/venv/data'\n",
    "filename = filepath +'/diabetes.csv'\n",
    "dataset = load_csv(filename)\n",
    "dataset = dataset[1:]              #removing headers \n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "    \n",
    "# evaluate algorithm\n",
    "split = 0.6\n",
    "accuracy = evaluate_algorithm(dataset, zero_rule_algorithm_classification, split)\n",
    "print('Accuracy: %.3f%%' % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018bec22-ae04-4fe1-a5f6-f9b33f6473a3",
   "metadata": {},
   "source": [
    "Notice\n",
    "how the name of the Zero Rule algorithm zero rule algorithm classification was passed\n",
    "as an argument to the evaluate algorithm() function. You can see how this test harness may\n",
    "be used again and again with diﬀerent algorithms. Running the example above prints out the\n",
    "accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4085b69b-0035-48cf-8573-fa8014e89954",
   "metadata": {},
   "source": [
    "### Cross-Validation Algorithm Test Harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc01f0b9-91c9-4826-a7db-2f941c0bab20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "lis = [[1,2],[3,4],[5,6,7]]\n",
    "lis = sum(lis,[0])\n",
    "print(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fcd0ed81-bd91-4a43-833d-5e188fd6df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate an algorithm using a cross-validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    #print('here folds: {0}'.format(folds))\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "            \n",
    "        print('ts1 {0}'.format(test_set[0]))\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b2ad303-640a-417f-ab92-66291e25c0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts1 [0.0, 93.0, 60.0, 25.0, 92.0, 28.7, 0.532, 22.0, None]\n",
      "ts1 [3.0, 158.0, 70.0, 30.0, 328.0, 35.5, 0.344, 35.0, None]\n",
      "ts1 [1.0, 89.0, 76.0, 34.0, 37.0, 31.2, 0.192, 23.0, None]\n",
      "ts1 [8.0, 109.0, 76.0, 39.0, 114.0, 27.9, 0.64, 31.0, None]\n",
      "ts1 [4.0, 118.0, 70.0, 0.0, 0.0, 44.5, 0.904, 26.0, None]\n",
      "ts1 [8.0, 176.0, 90.0, 34.0, 300.0, 33.7, 0.467, 58.0, None]\n",
      "ts1 [1.0, 140.0, 74.0, 26.0, 180.0, 24.1, 0.828, 23.0, None]\n",
      "Scores: [66.97247706422019, 55.96330275229357, 68.80733944954129, 63.30275229357798, 64.22018348623854, 67.88990825688074, 67.88990825688074]\n",
      "Mean Accuracy: 65.007%\n"
     ]
    }
   ],
   "source": [
    "# Test cross validation test harness\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filepath = '/Users/maheshwars/Desktop/venv/data'\n",
    "filename = filepath +'/diabetes.csv'\n",
    "dataset = load_csv(filename)\n",
    "dataset = dataset[1:] \n",
    "\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "    \n",
    "# evaluate algorithm\n",
    "n_folds = 7\n",
    "scores = evaluate_algorithm(dataset, zero_rule_algorithm_classification, n_folds)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68261772-75a2-4ff1-8441-729eedaef847",
   "metadata": {},
   "source": [
    "# **Simple Linear Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c93f8ef-bb29-4f15-b89c-3bd4a68667cb",
   "metadata": {},
   "source": [
    "### Calculate Mean and Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a30951f-6da2-4002-a89d-f440f31e567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean value of a list of numbers\n",
    "def mean(values):\n",
    "    return sum(values) / float(len(values))\n",
    "\n",
    "# Calculate the variance of a list of numbers\n",
    "def variance(values, mean):\n",
    "    return sum([(x-mean)**2 for x in values])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ebc590-e4b5-4620-ab5d-36e75c43942e",
   "metadata": {},
   "source": [
    "### Calculate Covariance\n",
    "\n",
    "The covariance of two groups of numbers describes how those numbers change together. Co-\n",
    "variance is a generalization of correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dedc9cc0-cc0c-425e-9ea4-56801257a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate covariance between x and y\n",
    "def covariance(x, mean_x, y, mean_y):\n",
    "    covar = 0.0\n",
    "    for i in range(len(x)):\n",
    "        covar += (x[i] - mean_x) * (y[i] - mean_y)\n",
    "    return covar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63719a57-9ae8-4fab-a0bc-95e9468def74",
   "metadata": {},
   "source": [
    "## Estimate Coeﬃcients\n",
    "y = B0 + B1*x\n",
    "\n",
    "B1 = covarience(x,y)/variance(x) <======> **W = summation(x(i).y(i)) / summation(x(i)^2)**\n",
    "\n",
    "B2 = mean(y) - B1*mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6865a900-f15e-4137-b0b4-d5380f7a9762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: B0=0.400, B1=0.800\n"
     ]
    }
   ],
   "source": [
    "# Calculate coefficients\n",
    "def coefficients(dataset):\n",
    "    x = [row[0] for row in dataset]\n",
    "    y = [row[1] for row in dataset]\n",
    "    x_mean, y_mean = mean(x), mean(y)\n",
    "    b1 = covariance(x, x_mean, y, y_mean) / variance(x, x_mean)\n",
    "    b0 = y_mean - b1 * x_mean\n",
    "    return [b0, b1]\n",
    "    \n",
    "# calculate coefficients\n",
    "dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]\n",
    "b0, b1 = coefficients(dataset)\n",
    "print('Coefficients: B0=%.3f, B1=%.3f' % (b0, b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4073f49d-06ce-41ab-84dc-7147f5fae356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficeints 0.39999999999999947, 0.8\n",
      "[1.1999999999999995, 1.9999999999999996, 3.5999999999999996, 2.8, 4.3999999999999995]\n",
      "RMSE: 0.693\n"
     ]
    }
   ],
   "source": [
    "# Evaluate regression algorithm on training dataset\n",
    "def evaluate_algorithm(dataset, algorithm):\n",
    "    test_set = list()\n",
    "    for row in dataset:\n",
    "        row_copy = list(row)\n",
    "        row_copy[-1] = None\n",
    "        test_set.append(row_copy)\n",
    "    predicted = algorithm(dataset, test_set)\n",
    "    print(predicted)\n",
    "    actual = [row[-1] for row in dataset]\n",
    "    rmse = rmse_metric(actual, predicted)\n",
    "    return rmse\n",
    "\n",
    "# Simple linear regression algorithm\n",
    "def simple_linear_regression(train, test):\n",
    "    predictions = list()\n",
    "    b0, b1 = coefficients(train)\n",
    "    print('coefficeints {0}, {1}'.format(b0,b1))\n",
    "    for row in test:\n",
    "        yhat = b0 + b1 * row[0]\n",
    "        predictions.append(yhat)\n",
    "    return predictions\n",
    "# Test simple linear regression\n",
    "dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]\n",
    "rmse = evaluate_algorithm(dataset, simple_linear_regression)\n",
    "print('RMSE: %.3f' % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356f13b3-06de-4eb7-a92e-aac4a717687b",
   "metadata": {},
   "source": [
    "### Insurance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d06366a-787c-49b7-ac3f-8a2129902ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "767e82c2-143c-4de2-bc04-43ba8348168d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** No CODEPAGE record, no encoding_override: will use 'iso-8859-1'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>108</td>\n",
       "      <td>392.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>46.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>15.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>124</td>\n",
       "      <td>422.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>119.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>57</td>\n",
       "      <td>170.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23</td>\n",
       "      <td>56.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14</td>\n",
       "      <td>77.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     X      Y\n",
       "0  108  392.5\n",
       "1   19   46.2\n",
       "2   13   15.7\n",
       "3  124  422.2\n",
       "4   40  119.4\n",
       "5   57  170.9\n",
       "6   23   56.9\n",
       "7   14   77.5"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file into a DataFrame\n",
    "filepath = '/Users/maheshwars/Desktop/venv/data'\n",
    "filename = filepath +'/insurance.xls'\n",
    "df = pd.read_excel(filename, engine='xlrd')\n",
    "\n",
    "# Print the DataFrame\n",
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "810bc70a-013d-4cde-bb4d-9338d75f725e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([108. , 392.5])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df.values\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b27130dc-2268-42c8-ad80-aea63da09a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficeints 28.228654849397856, 3.3175561991590437\n",
      "RMSE: 33.630\n"
     ]
    }
   ],
   "source": [
    "# Evaluate an algorithm using a train/test split\n",
    "def evaluate_algorithm(dataset, algorithm, split, *args):\n",
    "    train, test = train_test_split(dataset, split)\n",
    "    test_set = list()\n",
    "    for row in test:\n",
    "        row_copy = list(row)\n",
    "        row_copy[-1] = None\n",
    "        test_set.append(row_copy)\n",
    "    predicted = algorithm(train, test_set, *args)\n",
    "    actual = [row[-1] for row in test]\n",
    "    rmse = rmse_metric(actual, predicted)\n",
    "    return rmse\n",
    "    \n",
    "# evaluate algorithm\n",
    "seed(1)\n",
    "split = 0.6\n",
    "rmse = evaluate_algorithm(dataset, simple_linear_regression, split)\n",
    "print('RMSE: %.3f' % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4244458b-e52f-48fa-8f2d-e168e3977974",
   "metadata": {},
   "source": [
    "# **Multivariate Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "51e05d4e-0de5-4413-b368-080ee0f5c4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "    yhat = coefficients[0]\n",
    "    for i in range(len(row)-1):\n",
    "        yhat += coefficients[i + 1] * row[i]\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5502270b-fa58-4e28-bc3d-ea68f710a164",
   "metadata": {},
   "source": [
    "The first coeﬃcient in is always the intercept, also called the bias or b0 as it is standalone and\n",
    "not responsible for a specific input value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb8c1d6-d0fd-4053-9073-489eacb81122",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c47f8317-16bd-4f5f-8cf2-da3b63726c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate linear regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "    coef = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            yhat = predict(row, coef)\n",
    "            error = yhat - row[-1]\n",
    "            sum_error += error**2\n",
    "            coef[0] = coef[0] - l_rate * error #updated without an input as it is not associated with a specific input value\n",
    "            for i in range(len(row)-1):\n",
    "                coef[i + 1] = coef[i + 1] - l_rate * error * row[i] # #updated without an input (row[i])\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "155e7811-98cf-4976-bb18-1fb20321e477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.001, error=46.236\n",
      ">epoch=1, lrate=0.001, error=41.305\n",
      ">epoch=2, lrate=0.001, error=36.930\n",
      ">epoch=3, lrate=0.001, error=33.047\n",
      ">epoch=4, lrate=0.001, error=29.601\n",
      ">epoch=5, lrate=0.001, error=26.543\n",
      ">epoch=6, lrate=0.001, error=23.830\n",
      ">epoch=7, lrate=0.001, error=21.422\n",
      ">epoch=8, lrate=0.001, error=19.285\n",
      ">epoch=9, lrate=0.001, error=17.389\n",
      ">epoch=10, lrate=0.001, error=15.706\n",
      ">epoch=11, lrate=0.001, error=14.213\n",
      ">epoch=12, lrate=0.001, error=12.888\n",
      ">epoch=13, lrate=0.001, error=11.712\n",
      ">epoch=14, lrate=0.001, error=10.668\n",
      ">epoch=15, lrate=0.001, error=9.742\n",
      ">epoch=16, lrate=0.001, error=8.921\n",
      ">epoch=17, lrate=0.001, error=8.191\n",
      ">epoch=18, lrate=0.001, error=7.544\n",
      ">epoch=19, lrate=0.001, error=6.970\n",
      ">epoch=20, lrate=0.001, error=6.461\n",
      ">epoch=21, lrate=0.001, error=6.009\n",
      ">epoch=22, lrate=0.001, error=5.607\n",
      ">epoch=23, lrate=0.001, error=5.251\n",
      ">epoch=24, lrate=0.001, error=4.935\n",
      ">epoch=25, lrate=0.001, error=4.655\n",
      ">epoch=26, lrate=0.001, error=4.406\n",
      ">epoch=27, lrate=0.001, error=4.186\n",
      ">epoch=28, lrate=0.001, error=3.990\n",
      ">epoch=29, lrate=0.001, error=3.816\n",
      ">epoch=30, lrate=0.001, error=3.662\n",
      ">epoch=31, lrate=0.001, error=3.525\n",
      ">epoch=32, lrate=0.001, error=3.404\n",
      ">epoch=33, lrate=0.001, error=3.296\n",
      ">epoch=34, lrate=0.001, error=3.200\n",
      ">epoch=35, lrate=0.001, error=3.115\n",
      ">epoch=36, lrate=0.001, error=3.040\n",
      ">epoch=37, lrate=0.001, error=2.973\n",
      ">epoch=38, lrate=0.001, error=2.914\n",
      ">epoch=39, lrate=0.001, error=2.862\n",
      ">epoch=40, lrate=0.001, error=2.815\n",
      ">epoch=41, lrate=0.001, error=2.773\n",
      ">epoch=42, lrate=0.001, error=2.737\n",
      ">epoch=43, lrate=0.001, error=2.704\n",
      ">epoch=44, lrate=0.001, error=2.675\n",
      ">epoch=45, lrate=0.001, error=2.650\n",
      ">epoch=46, lrate=0.001, error=2.627\n",
      ">epoch=47, lrate=0.001, error=2.607\n",
      ">epoch=48, lrate=0.001, error=2.589\n",
      ">epoch=49, lrate=0.001, error=2.573\n",
      "[0.22998234937311363, 0.8017220304137576]\n"
     ]
    }
   ],
   "source": [
    "# Calculate coefficients\n",
    "dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]\n",
    "l_rate = 0.001\n",
    "n_epoch = 50\n",
    "coef = coefficients_sgd(dataset, l_rate, n_epoch)\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b1509-809b-4156-9aa6-1466d44f0765",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
