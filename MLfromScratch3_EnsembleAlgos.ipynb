{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "628020b7-86fd-4557-ba75-25a6a71a1dac",
   "metadata": {},
   "source": [
    "# __Ensemble Algorithms__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff05c563-11d9-467d-9231-5421cb80c364",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## __Bootstrap Aggregation__\n",
    "\n",
    "Decision trees are a simple and powerful predictive modeling technique, but they suﬀer from\n",
    "**high-variance. This means that trees can get very diﬀerent results given diﬀerent training data**.\n",
    "A technique to make decision trees more robust and to achieve better performance is called\n",
    "bootstrap aggregation or bagging for short.\n",
    "\n",
    "A bootstrap is a sample of a dataset with replacement. This means that a new dataset is created\n",
    "from a random sample of an existing dataset where a given row may be selected and added\n",
    "more than once to the sample. It is a useful approach to use when estimating values such as\n",
    "the mean for a broader dataset, when you only have a limited dataset available. By creating\n",
    "samples of your dataset and estimating the mean from those samples, you can take the average\n",
    "of those estimates and get a better idea of the true mean of the underlying problem.\n",
    "This same approach can be used with machine learning algorithms that have a high variance.\n",
    "\n",
    "**A separate model is trained on each bootstrap sample of data and the average output of those models used to make predictions.\n",
    "This technique is called bootstrap aggregation or bagging** for short. Variance means that an\n",
    "algorithm’s performance is sensitive to the training data, with __high variance suggesting that the\n",
    "more the training data is changed, the more the performance of the algorithm will vary__.\n",
    "\n",
    "The performance of high variance machine learning algorithms like unpruned decision trees\n",
    "can be improved by training many trees and taking the average of their predictions. Results are\n",
    "often better than a single decision tree. **Another benefit of bagging in addition to improved\n",
    "performance is that the bagged decision trees cannot overfit the problem. Trees can continue to\n",
    "be added until a maximum in performance is achieved.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a15a71a-c6d5-4294-96c9-ee26010b5dbe",
   "metadata": {},
   "source": [
    "### Bootstrap Resample\n",
    "\n",
    "We can create a\n",
    "new sample of a dataset by randomly selecting rows from the dataset and adding them to a new\n",
    "list. We can repeat this for a fixed number of rows or until the size of the new dataset matches\n",
    "a ratio of the size of the original dataset.\n",
    "We can allow sampling with replacement by not removing the row that was selected so that\n",
    "it is available for future selections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "490faeed-532b-4d70-ab64-20387025ea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of subsampling a dataste\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from math import sqrt\n",
    "from math import exp\n",
    "from csv import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbb6f44a-c733-4b96-98ec-af4fa13acbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random subsample from the dataset with replacement\n",
    "def subsample(dataset, ratio=1.0):\n",
    "    sample = list()\n",
    "    n_sample = round(len(dataset) * ratio)\n",
    "    while len(sample) < n_sample:\n",
    "        index = randrange(len(dataset))\n",
    "        sample.append(dataset[index])\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abf6eb8e-6895-4106-b2a8-2b5ee781939b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Mean: 4.500\n",
      "Samples=1, Estimated Mean: 4.000\n",
      "Samples=10, Estimated Mean: 4.700\n",
      "Samples=100, Estimated Mean: 4.570\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean of a list of numbers EXAMPLE\n",
    "def mean(numbers):\n",
    "    return sum(numbers) / float(len(numbers))\n",
    "    \n",
    "# Test subsampling a dataset\n",
    "seed(1)\n",
    "\n",
    "# True mean\n",
    "dataset = [[randrange(10)] for i in range(20)]\n",
    "print('True Mean: %.3f' % mean([row[0] for row in dataset]))\n",
    "\n",
    "# Estimated means\n",
    "ratio = 0.10\n",
    "for size in [1, 10, 100]:\n",
    "    sample_means = list()\n",
    "    for i in range(size):\n",
    "        sample = subsample(dataset, ratio)\n",
    "        sample_mean = mean([row[0] for row in sample])\n",
    "        sample_means.append(sample_mean)\n",
    "    print('Samples=%d, Estimated Mean: %.3f' % (size, mean(sample_means)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fdf925-da8c-482f-9c0b-2e65435be905",
   "metadata": {},
   "source": [
    "### Sonar Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "782b6f49-f307-49e2-a366-ee0cfccee3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging Algorithm on the Sonar dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "        \n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "    \n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for _ in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "    \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "    \n",
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset):\n",
    "    left, right = list(), list()\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "    \n",
    "# Calculate the Gini index for a split dataset\n",
    "def gini_index(groups, classes):\n",
    "    # count all samples at split point\n",
    "    n_instances = float(sum([len(group) for group in groups]))\n",
    "    # sum weighted Gini index for each group\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        # avoid divide by zero\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        # score the group based on the score for each class\n",
    "        for class_val in classes:\n",
    "            p = [row[-1] for row in group].count(class_val) / size\n",
    "            score += p * p\n",
    "        # weight the group score by its relative size\n",
    "        gini += (1.0 - score) * (size / n_instances)\n",
    "    return gini\n",
    "\n",
    "# Select the best split point for a dataset\n",
    "def get_split(dataset):\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    for index in range(len(dataset[0])-1):\n",
    "        for row in dataset:\n",
    "        # for i in range(len(dataset)):\n",
    "            # row = dataset[randrange(len(dataset))]\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini = gini_index(groups, class_values)\n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "\n",
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    return max(set(outcomes), key=outcomes.count)\n",
    "    \n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    \n",
    "    # check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return  \n",
    "\n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    \n",
    "    # process left child\n",
    "    if len(left) <= min_size or len(set([row[-1] for row in left]))==1:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left)       \n",
    "        split(node['left'], max_depth, min_size, depth+1)\n",
    "        \n",
    "    # process right child\n",
    "    if len(right) <= min_size or len(set([row[-1] for row in right]))==1:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right)\n",
    "        split(node['right'], max_depth, min_size, depth+1)\n",
    "\n",
    "# Build a decision tree\n",
    "def build_tree(train, max_depth, min_size):\n",
    "    root = get_split(train)\n",
    "    split(root, max_depth, min_size, 1)\n",
    "    return root\n",
    "\n",
    "# Make a prediction with a decision tree\n",
    "def predict(node, row):\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    "\n",
    "# Make a prediction with a list of bagged trees\n",
    "def bagging_predict(trees, row):\n",
    "    predictions = [predict(tree, row) for tree in trees]\n",
    "    return max(set(predictions), key=predictions.count)\n",
    "\n",
    "# Bootstrap Aggregation Algorithm\n",
    "def bagging(train, test, max_depth, min_size, sample_size, n_trees):\n",
    "    trees = list()\n",
    "    for _ in range(n_trees):\n",
    "        sample = subsample(train, sample_size)\n",
    "        tree = build_tree(sample, max_depth, min_size)\n",
    "        trees.append(tree)\n",
    "    predictions = [bagging_predict(trees, row) for row in test]\n",
    "    return(predictions)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bdfc0f48-6d4b-4cf0-8d0d-e090f98bc958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file /Users/maheshwars/Desktop/venv/data/sonar/sonar_data.csv with 208 rows and 61 columns\n",
      "Trees: 1\n",
      "Scores: [87.8048780487805, 65.85365853658537, 65.85365853658537, 65.85365853658537, 73.17073170731707]\n",
      "Mean Accuracy: 71.707%\n",
      "Trees: 5\n",
      "Scores: [60.97560975609756, 80.48780487804879, 78.04878048780488, 82.92682926829268, 63.41463414634146]\n",
      "Mean Accuracy: 73.171%\n",
      "Trees: 10\n",
      "Scores: [58.536585365853654, 78.04878048780488, 80.48780487804879, 87.8048780487805, 65.85365853658537]\n",
      "Mean Accuracy: 74.146%\n",
      "Trees: 50\n",
      "Scores: [65.85365853658537, 75.60975609756098, 80.48780487804879, 73.17073170731707, 82.92682926829268]\n",
      "Mean Accuracy: 75.610%\n",
      "Trees: 100\n",
      "Scores: [85.36585365853658, 87.8048780487805, 75.60975609756098, 73.17073170731707, 73.17073170731707]\n",
      "Mean Accuracy: 79.024%\n"
     ]
    }
   ],
   "source": [
    "# Test bagging on the sonar dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filepath = '/Users/maheshwars/Desktop/venv/data/sonar'\n",
    "filename = filepath +'/sonar_data.csv'\n",
    "dataset = load_csv(filename)\n",
    "print('Loaded data file {0} with {1} rows and {2} columns'.format(filename, len(dataset),\n",
    "                                                                  len(dataset[0])))\n",
    "\n",
    "# convert string attributes to integers\n",
    "for i in range(len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "    \n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "max_depth = 6\n",
    "min_size = 2\n",
    "sample_size = 0.50\n",
    "\n",
    "for n_trees in [1, 5, 10, 50,100]:\n",
    "    scores = evaluate_algorithm(dataset, bagging, n_folds, max_depth, min_size, sample_size, n_trees)\n",
    "    print('Trees: %d' % n_trees)\n",
    "    print('Scores: %s' % scores)\n",
    "    print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ed200f-a65e-42dd-8bc7-57c9a47a2750",
   "metadata": {},
   "source": [
    "__A diﬃculty of this method is that even though deep trees are constructed, the bagged trees\n",
    "that are created are very similar. In turn, the predictions made by these trees are also similar,\n",
    "and the high variance we desire among the trees trained on diﬀerent samples of the training\n",
    "dataset is diminished.\n",
    "This is because of the greedy algorithm used in the construction of the trees selecting the\n",
    "same or similar split points.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d92173-339f-4828-a25a-058c76398f89",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Random Forest**\n",
    "\n",
    "Decision trees involve the greedy selection of the best split point from the dataset at each step.\n",
    "This algorithm makes decision trees susceptible to high variance if they are not pruned. This\n",
    "high variance can be harnessed and reduced by creating multiple trees with diﬀerent samples\n",
    "of the training dataset (diﬀerent views of the problem) and combining their predictions. This\n",
    "approach is called bootstrap aggregation or bagging for short.\n",
    "\n",
    "A limitation of bagging is that the same greedy algorithm is used to create each tree, meaning\n",
    "that it is likely that **the same or very similar split points will be chosen in each tree** making\n",
    "the diﬀerent trees very similar (trees will be correlated). This, in turn, makes their predictions\n",
    "similar, mitigating the variance originally sought.\n",
    "\n",
    "We can force the decision trees to be diﬀerent by limiting the features (columns) that the\n",
    "greedy algorithm can evaluate at each split point when creating the tree. This is called the\n",
    "Random Forest algorithm. Like bagging, multiple samples of the training dataset are taken and\n",
    "a diﬀerent tree trained on each. The diﬀerence is that at each point a split is made in the data\n",
    "and added to the tree, only a fixed subset of attributes can be considered. For classification\n",
    "problems, the type of problems we will look at in this tutorial, the number of attributes to be\n",
    "considered for the split is limited to the square root of the number of input features.\n",
    "\n",
    "    num features for split = total input features         \n",
    "The result of this one small change are trees that are more diﬀerent from each other\n",
    "(uncorrelated) resulting in predictions that are more diverse and a combined prediction that\n",
    "often has better performance than a single tree or bagging alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf7e597-7bba-4aa4-b438-d51454f44da3",
   "metadata": {},
   "source": [
    "## Calculating Splits\n",
    "\n",
    "Finding the best split point in a decision tree involves evaluating the cost of each value in\n",
    "the training dataset for each input variable. For bagging and random forest, this procedure\n",
    "is executed upon a sample of the training dataset, made with replacement. Sampling with\n",
    "replacement means that the same row may be chosen and added to the sample more than once.\n",
    "We can update this procedure for Random Forest. Instead of enumerating all values for\n",
    "input attributes in search of the split with the lowest cost, we can create a sample of the input\n",
    "attributes to consider. This sample of input attributes can be chosen randomly and without replacement, meaning that each input attribute needs to only be considered once when looking\n",
    "for the split point with the lowest cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "832d0f04-453d-4b60-b070-376b0f0d952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best split point for a dataset\n",
    "def get_split(dataset, n_features):\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    features = list()\n",
    "    while len(features) < n_features:\n",
    "        index = randrange(len(dataset[0])-1)\n",
    "        if index not in features:\n",
    "            features.append(index)\n",
    "    for index in features:\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini = gini_index(groups, class_values)\n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "\n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, n_features, depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    \n",
    "    # check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    \n",
    "    # process left child\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left, n_features)\n",
    "        split(node['left'], max_depth, min_size, n_features, depth+1)\n",
    "    \n",
    "    # process right child\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right, n_features)\n",
    "        split(node['right'], max_depth, min_size, n_features, depth+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882cd678-709a-4ac8-8876-aeabb966c2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a decision tree\n",
    "def build_tree(train, max_depth, min_size, n_features): # n_features #new\n",
    "    root = get_split(train, n_features)\n",
    "    split(root, max_depth, min_size, n_features, 1)\n",
    "    return root\n",
    "\n",
    "# Random Forest Algorithm\n",
    "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
    "    trees = list()\n",
    "    for _ in range(n_trees):\n",
    "        sample = subsample(train, sample_size)\n",
    "        tree = build_tree(sample, max_depth, min_size, n_features)\n",
    "        trees.append(tree)\n",
    "    predictions = [bagging_predict(trees, row) for row in test]\n",
    "    return(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c1d5b7e-6d88-41e2-93c3-6c472b72cdb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file /Users/maheshwars/Desktop/venv/data/sonar/sonar_data.csv with 208 rows and 61 columns\n",
      "Trees: 1\n",
      "Scores: [56.09756097560976, 63.41463414634146, 60.97560975609756, 58.536585365853654, 73.17073170731707]\n",
      "Mean Accuracy: 62.439%\n",
      "Trees: 5\n",
      "Scores: [70.73170731707317, 58.536585365853654, 85.36585365853658, 75.60975609756098, 63.41463414634146]\n",
      "Mean Accuracy: 70.732%\n",
      "Trees: 10\n",
      "Scores: [82.92682926829268, 75.60975609756098, 97.5609756097561, 80.48780487804879, 68.29268292682927]\n",
      "Mean Accuracy: 80.976%\n"
     ]
    }
   ],
   "source": [
    "# Test the random forest algorithm on sonar dataset\n",
    "seed(2)\n",
    "# load and prepare data\n",
    "filepath = '/Users/maheshwars/Desktop/venv/data/sonar'\n",
    "filename = filepath +'/sonar_data.csv'\n",
    "dataset = load_csv(filename)\n",
    "print('Loaded data file {0} with {1} rows and {2} columns'.format(filename, len(dataset),\n",
    "                                                                  len(dataset[0])))\n",
    "\n",
    "# convert string attributes to integers\n",
    "for i in range(0, len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "    \n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "max_depth = 10\n",
    "min_size = 1\n",
    "sample_size = 1.0\n",
    "n_features = int(sqrt(len(dataset[0])-1))\n",
    "for n_trees in [1, 5, 10]:\n",
    "    scores = evaluate_algorithm(dataset, random_forest, n_folds, max_depth, min_size,\n",
    "    sample_size, n_trees, n_features)\n",
    "    print('Trees: %d' % n_trees)\n",
    "    print('Scores: %s' % scores)\n",
    "    print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db61366-6bce-4f71-8840-a85b13d40d90",
   "metadata": {},
   "source": [
    "# **Stacked Generalisation Algorithm**\n",
    "\n",
    "Stacked Generalization is an ensemble algorithm where a new model is trained to __combine the\n",
    "predictions from two or more models__ already trained or your dataset. The predictions from the\n",
    "existing models or submodels **are combined using a new model**, and as such stacking is often\n",
    "referred to as **blending**, as the predictions from submodels are blended together.\n",
    "\n",
    "It is typical to use **a simple linear method to combine the predictions for submodels. Examples\n",
    "include: simple averaging called voting, a weighted sum using linear regression. and logistic\n",
    "regression.** Models that have their predictions combined must have skill on the problem, but\n",
    "do not need to be the best possible models. This means that you do not need to tune the\n",
    "__submodels__ intently, **as long as the model shows some advantage over a baseline prediction.\n",
    "It is important that submodels produce diﬀerent predictions, so-called uncorrelated predicions.** Stacking works best when the predictions that are combined are all skillful, but skillful\n",
    "in diﬀerent ways. This may be achieved by **using algorithms that use very diﬀerent internal\n",
    "representations (trees compared to instances) and/or models trained on diﬀerent representations\n",
    "or projections of the training data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13b79f0-92c9-40b4-afff-9d0db120ed39",
   "metadata": {},
   "source": [
    "## Submodels and Aggregator\n",
    "\n",
    "We are going to use two models as submodels for stacking and a linear model as the aggregator\n",
    "model.\n",
    "This part is divided into 3 sections:\n",
    "1. Submodel #1: k-Nearest Neighbors.\n",
    "2. Submodel #2: Perceptron.\n",
    "3. Aggregator Model: Logistic Regression.\n",
    "   \n",
    "Each model will be described in terms of the functions used to train the model and a function\n",
    "used to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad77115-d693-47e8-98a3-8e82bf640baf",
   "metadata": {},
   "source": [
    "### Submodel #1: k-Nearest Neighbors\n",
    "The k-Nearest Neighbors algorithm or KNN uses the entire training dataset as the model.\n",
    "Therefore training the model involves retaining the training dataset. Below is a function named\n",
    "knn model() that does just this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2646468-c1bc-4802-b699-b37c213f71d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the KNN model\n",
    "def knn_model(train):\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c4bac67e-4c0f-480a-94b4-82fa853afc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "    distance = 0.0\n",
    "    for i in range(len(row1)-1):\n",
    "        distance += (row1[i] - row2[i])**2\n",
    "    return sqrt(distance)\n",
    "\n",
    "# Locate neighbors for a new row\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "    distances = list()\n",
    "    for train_row in train:\n",
    "        dist = euclidean_distance(test_row, train_row)\n",
    "        distances.append((train_row, dist))\n",
    "    distances.sort(key=lambda tup: tup[1])\n",
    "    neighbors = list()\n",
    "    for i in range(num_neighbors):\n",
    "        neighbors.append(distances[i][0])\n",
    "    return neighbors\n",
    "\n",
    "# Make a prediction with KNN\n",
    "def knn_predict(model, test_row, num_neighbors=2):\n",
    "    neighbors = get_neighbors(model, test_row, num_neighbors)\n",
    "    output_values = [row[-1] for row in neighbors]\n",
    "    prediction = max(set(output_values), key=output_values.count)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b506b14a-2a6a-49c9-9dee-fb356e24083b",
   "metadata": {},
   "source": [
    "### Submodel #2: Perceptron\n",
    "\n",
    "The model for the Perceptron algorithm is a set of weights learned from the training data. In\n",
    "order to train the weights, many predictions need to be made on the training data in order\n",
    "to calculate error values. Therefore, both model training and prediction require a function for\n",
    "prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3af48eab-0012-487c-ab2b-206740b33af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with weights\n",
    "def perceptron_predict(model, row):\n",
    "    activation = model[0] #bias\n",
    "    for i in range(len(row)-1):\n",
    "        activation += model[i + 1] * row[i]\n",
    "    return 1.0 if activation >= 0.0 else 0.0\n",
    "\n",
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def perceptron_model(train, l_rate=0.01, n_epoch=5000):\n",
    "    weights = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            prediction = perceptron_predict(weights, row)\n",
    "            error = row[-1] - prediction\n",
    "            weights[0] = weights[0] + l_rate * error\n",
    "            for i in range(len(row)-1):\n",
    "                weights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "    return weights    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c4536c-fd69-47c1-ba44-8eada7fd40ec",
   "metadata": {},
   "source": [
    "### Aggregator Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6eafb8b7-5d76-4933-8eda-bb595fe9dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with coefficients\n",
    "def logistic_regression_predict(model, row):\n",
    "    yhat = model[0]\n",
    "    for i in range(len(row)-1):\n",
    "        yhat += model[i + 1] * row[i]\n",
    "    return 1.0 / (1.0 + exp(-yhat))\n",
    "\n",
    "# Estimate logistic regression coefficients using stochastic gradient descent\n",
    "def logistic_regression_model(train, l_rate=0.01, n_epoch=5000):\n",
    "    coef = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            yhat = logistic_regression_predict(coef, row)\n",
    "            error = row[-1] - yhat\n",
    "            coef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
    "        for i in range(len(row)-1):\n",
    "            coef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e538ace-c115-435b-a410-0fc090b5df01",
   "metadata": {},
   "source": [
    "### Combining Predictions\n",
    "\n",
    "For a machine learning algorithm, learning how to combine predictions is much the same as\n",
    "learning from a training dataset. A new training dataset can be constructed from the predictions\n",
    "of the submodels, as follows:\n",
    "\n",
    "* Each row represents one row in the training dataset.\n",
    "* The first column contains predictions for each row in the training dataset made by the\n",
    "first submodel, such as k-Nearest Neighbors.\n",
    "* The second column contains predictions for each row in the training dataset made by the\n",
    "second submodel, such as the Perceptron algorithm.\n",
    "* The third column contains the expected output value for the row in the training dataset.\n",
    "\n",
    "\n",
    "The function takes a list of models as input; these are used to make predictions. The function\n",
    "also takes a list of functions as input, one function used to make a prediction for each model.\n",
    "Finally, a single row from the training dataset is included. A new row is constructed one column\n",
    "at a time. Predictions are calculated using each model and the row of training data. The\n",
    "expected output value from the training dataset row is then added as the last column to the\n",
    "row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "68077904-ac79-4bc8-a172-5657f37fe964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with submodels and construct a new stacked row\n",
    "def to_stacked_row(models, predict_list, row):\n",
    "    stacked_row = list()\n",
    "    for i in range(len(models)):\n",
    "        prediction = predict_list[i](models[i], row)\n",
    "        stacked_row.append(prediction)\n",
    "    stacked_row.append(row[-1])\n",
    "    return stacked_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ae17c-e3b8-41eb-9d0f-17e62817d1c2",
   "metadata": {},
   "source": [
    "On some predictive modeling problems, it is possible to get an even larger boost by training\n",
    "the aggregated model on both the training row and the predictions made by submodels. This\n",
    "improvement gives the aggregator model the context of all the data in the training row to help\n",
    "determine how and when to best combine the predictions of the submodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd2a4615-0fca-4954-b7ac-991001938130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with sub-models and construct a new stacked row\n",
    "def to_stacked_row(models, predict_list, row):\n",
    "    stacked_row = list()\n",
    "    for i in range(len(models)):\n",
    "        prediction = predict_list[i](models[i], row)\n",
    "        stacked_row.append(prediction)\n",
    "    stacked_row.append(row[-1])\n",
    "    return row[0:len(row)-1] + stacked_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ced57d-decf-4614-8c3e-9ac21168ca61",
   "metadata": {},
   "source": [
    "A new function name stacking() is developed. This function does 4 things:\n",
    "1. It first trains a list of models (KNN and Perceptron).\n",
    "2. It then uses the models to make predictions and create a new stacked dataset.\n",
    "3. It then trains an aggregator model (logistic regression) on the stacked dataset.\n",
    "4. It then uses the submodels and aggregator model to make predictions on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f9d8cf4d-e66c-4d09-abfe-65f56693a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked Generalization Algorithm\n",
    "def stacking(train, test):\n",
    "    model_list = [knn_model, perceptron_model]\n",
    "    predict_list = [knn_predict, perceptron_predict]\n",
    "    models = list()\n",
    "    for i in range(len(model_list)):\n",
    "        model = model_list[i](train)\n",
    "        models.append(model)\n",
    "    stacked_dataset = list()\n",
    "    for row in train:\n",
    "        stacked_row = to_stacked_row(models, predict_list, row)\n",
    "        stacked_dataset.append(stacked_row)\n",
    "        stacked_model = logistic_regression_model(stacked_dataset)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        stacked_row = to_stacked_row(models, predict_list, row)\n",
    "        stacked_dataset.append(stacked_row)\n",
    "        prediction = logistic_regression_predict(stacked_model, stacked_row)\n",
    "        prediction = round(prediction)\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d8d3476e-0426-4ff2-b88a-42fda1ac8704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file /Users/maheshwars/Desktop/venv/data/sonar/sonar_data.csv with 208 rows and 61 columns\n",
      "Scores: [78.26086956521739, 69.56521739130434, 42.028985507246375]\n",
      "Mean Accuracy: 63.285%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test stacking on the sonar dataset\n",
    "seed(2)\n",
    "# load and prepare data\n",
    "filepath = '/Users/maheshwars/Desktop/venv/data/sonar'\n",
    "filename = filepath +'/sonar_data.csv'\n",
    "dataset = load_csv(filename)\n",
    "print('Loaded data file {0} with {1} rows and {2} columns'.format(filename, len(dataset),\n",
    "                                                                  len(dataset[0])))\n",
    "\n",
    "# convert string attributes to integers\n",
    "for i in range(len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "    \n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "n_folds = 3\n",
    "scores = evaluate_algorithm(dataset, stacking, n_folds)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bba69b0-7943-484e-8c1b-1c95d22f213c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
