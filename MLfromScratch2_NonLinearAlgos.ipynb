{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7f2edf4-ddd0-4914-a427-b140ccafd74a",
   "metadata": {},
   "source": [
    "# **Non Linear Algorithms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbbb360f-df1f-4e6f-ad89-d6e365add13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression With Stochastic Gradient Descent for Wine Quality\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a85b8887-126b-4bf0-98af-ad6849a61f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if(len(row)>4 and len([val for val in row if '\\\\' in val])>0):\n",
    "                row = [val.replace('\\\\','') for val in row]\n",
    "            if not row or len(row)<4 or (len([val for val in row if \n",
    "                                              ('apple' in val or 'outl0strokewidth0' in val or '}' in val)])>0):\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for _ in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "    \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2289d26-7f7a-42ba-a603-355566a28f90",
   "metadata": {},
   "source": [
    "# **Classification and Regression Trees**\n",
    "\n",
    "A node represents a single input variable (X) and a split point on that variable, assuming the\n",
    "variable is numeric. The leaf nodes (also called terminal nodes) of the tree contain an output\n",
    "variable (y) which is used to make a prediction. Once created, a tree can be navigated with a\n",
    "new row of data following each branch with the splits until a final prediction is made.\n",
    "\n",
    "Creating a binary decision tree is actually a process of dividing up the input space. A greedy\n",
    "approach is used called recursive binary splitting. This is a numerical procedure where all the\n",
    "values are lined up and diﬀerent split points are tried and tested using a cost function. The split\n",
    "with the best cost (lowest cost because we minimize cost) is selected. All input variables and all\n",
    "possible split points are evaluated and chosen in a greedy manner based on the cost function.\n",
    "\n",
    "* Regression: The cost function that is minimized to choose split points is the sum squared\n",
    "error across all training samples that fall within the rectangle.\n",
    "\n",
    "* Classification: The Gini cost function is used which provides an indication of how pure\n",
    "the nodes are, where node purity refers to how mixed the training data assigned to each\n",
    "node is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9df36b-0588-4a8f-8a25-eb8c82ed0185",
   "metadata": {},
   "source": [
    "## Gini index calculation\n",
    "\n",
    "Two groups are formed by the split on an attribute given by an index over a value, say A and B.\n",
    "\n",
    "lets say we have two classes (0 and 1) in our data and both the groups may contain the data from both the classes.\n",
    "\n",
    "A0 = # of rows in A from class 0\n",
    "\n",
    "A1 = # of rows in A from class 1\n",
    "\n",
    "B0 = # of rows in B from class 0\n",
    "\n",
    "B1 = # of rows in B from class 1\n",
    "\n",
    "\n",
    "\n",
    "--p is proportion\n",
    "\n",
    "pA0 = count(A0)/count(A);\n",
    "\n",
    "pA1 = count(A1)/count(A); \n",
    "\n",
    "pB0 = count(B0)/count(B); \n",
    "\n",
    "pB1 = count(B1)/count(B); \n",
    "\n",
    "\n",
    "gini_index_A = sum_over_classes (p_class (1-p_class)) = 1 - sum_over_class(p_class^2)\n",
    "\n",
    "gini_index_B = sum_over_classes (p_class (1-p_class)) = 1 - sum_over_class(p_class^2)\n",
    "\n",
    "\n",
    "\n",
    "final gini index of split is the weighted gini_index as per the group count.\n",
    "\n",
    "wA = count(A)/total_instances;\n",
    "\n",
    "wB = count(B)/total_instances;\n",
    "\n",
    "\n",
    "finally, gini index for a split on column - col[index] over a value is-\n",
    "\n",
    "GINI_INDEX = wA * (gini_index_A) + wB * (gini_index_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0c82ad5b-2057-4a03-83e7-ee0a0e04fecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Gini index for a split dataset\n",
    "def gini_index(groups, classes):\n",
    "    # count all samples at split point\n",
    "    n_instances = float(sum([len(group) for group in groups]))\n",
    "    \n",
    "    # sum weighted Gini index for each group\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        # avoid divide by zero\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        # score the group based on the score for each class\n",
    "        for class_val in classes:\n",
    "            p = [row[-1] for row in group].count(class_val) / size\n",
    "            score += p * p\n",
    "        # weight the group score by its relative size\n",
    "        gini += (1.0 - score) * (size / n_instances)\n",
    "    return gini\n",
    "\n",
    "# test Gini values\n",
    "print(gini_index([[[1, 1], [1, 0]], [[1, 1], [1, 0]]], [0, 1]))\n",
    "print(gini_index([[[1, 0], [1\n",
    "                            , 0]], [[1, 1], [1, 1]]], [0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163dc447-7eec-4172-978b-b3e6a2f0ae50",
   "metadata": {},
   "source": [
    "###  Splitting a dataset\n",
    "Splitting a dataset means separating a dataset into two lists of rows given the index of an\n",
    "attribute and a split value for that attribute. Once we have the two groups, we can then use\n",
    "our Gini score above to evaluate the cost of the split. Splitting a dataset involves iterating over\n",
    "each row, checking if the attribute value is below or above the split value and assigning it to the\n",
    "left or right group respectively. Below is a function named test split() that implements this\n",
    "procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94baa0ba-18bd-4fc6-99f3-7759cefdbda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset):\n",
    "    left, right = list(), list()\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb129ee1-cf9a-4beb-b5f7-f481711008ee",
   "metadata": {},
   "source": [
    "### Evaluating All Splits\n",
    "With the Gini function above and the test split function we now have everything we need to\n",
    "evaluate splits. Given a dataset, we must check every value on each attribute as a candidate\n",
    "split, evaluate the cost of the split and find the best possible split we could make. Once the\n",
    "best split is found, we can use it as a node in our decision tree.\n",
    "\n",
    "This is an exhaustive and greedy algorithm. We will use a dictionary to represent a node in\n",
    "the decision tree as we can store data by name. When selecting the best split and using it as a\n",
    "new node for the tree we will store the index of the chosen attribute, the value of that attribute\n",
    "by which to split and the two groups of data split by the chosen split point.\n",
    "Each group of data is its own small dataset of just those rows assigned to the left or right\n",
    "group by the splitting process. We might split each group again, recursively\n",
    "as we build out our decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aad80ec2-bf1f-49b7-9d81-b46fd4f76c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best split point for a dataset\n",
    "def get_split(dataset):\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    for index in range(len(dataset[0])-1):\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini = gini_index(groups, class_values)\n",
    "            #print('X%d < %.3f Gini=%.3f' % ((index+1), row[index], gini))\n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e3bae94e-0a83-49e3-8672-c5e63b6ba33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: [X1 < 7.445]\n"
     ]
    }
   ],
   "source": [
    "# Test getting the best split\n",
    "dataset = [[2.771244718,1.784783929,0],\n",
    "[1.728571309,1.169761413,0],\n",
    "[3.678319846,2.81281357,0],\n",
    "[3.961043357,2.61995032,0],\n",
    "[2.999208922,2.209014212,0],\n",
    "[7.497545867,3.162953546,1],\n",
    "[9.00220326,3.339047188,1],\n",
    "[7.444542326,0.476683375,1],\n",
    "[10.12493903,3.234550982,1],\n",
    "[6.642287351,3.319983761,0]]\n",
    "split = get_split(dataset)\n",
    "print('Split: [X%d < %.3f]' % ((split['index']+1), split['value']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8ff03f-d55b-4d50-8a0e-721078659469",
   "metadata": {},
   "source": [
    "## Build a Tree\n",
    "\n",
    "#### Terminal Nodes\n",
    "We need to decide when to stop growing a tree. We can do that using the depth and the number\n",
    "of rows that the node is responsible for in the training dataset.\n",
    "\n",
    "* Maximum Tree Depth. This is the maximum number of nodes from the root node of\n",
    "the tree. Once a maximum depth of the tree is met, we must stop adding new nodes.\n",
    "Deeper trees are more complex and are more likely to overfit the training data.\n",
    "\n",
    "* Minimum Node Records. This is the minimum number of training patterns that a\n",
    "given node is responsible for. Once at or below this minimum, we must stop splitting and\n",
    "adding new nodes. Nodes that account for too few training patterns are expected to be\n",
    "too specific and are likely to overfit the training data.\n",
    "\n",
    "\n",
    "These two approaches will be user-specified arguments to our tree building procedure. There\n",
    "is one more condition; it is possible to choose a split in which all rows belong to one group. In\n",
    "this case, we will be unable to continue splitting and adding child nodes as we will have no\n",
    "records to split on one side or another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "944dfc75-dfbf-43e5-b538-8063bac5c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    return max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    \n",
    "    # check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "\n",
    "\n",
    "    # if len(set([row[-1] for row in left]))==1:\n",
    "    #     print('LEFT PURE at depth {0}'.format(depth))\n",
    "        \n",
    "    # if len(set([row[-1] for row in right]))==1:\n",
    "    #     print('RIGHT PURE at depth {0}'.format(depth))   \n",
    "\n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    \n",
    "    # process left child\n",
    "    if len(left) <= min_size or len(set([row[-1] for row in left]))==1:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left)       \n",
    "        split(node['left'], max_depth, min_size, depth+1)\n",
    "        \n",
    "    # process right child\n",
    "    if len(right) <= min_size or len(set([row[-1] for row in right]))==1:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right)\n",
    "        split(node['right'], max_depth, min_size, depth+1)\n",
    "\n",
    "\n",
    "# Build a decision tree\n",
    "def build_tree(train, max_depth, min_size):\n",
    "    root = get_split(train)\n",
    "    split(root, max_depth, min_size, 1)\n",
    "    return root   \n",
    "\n",
    "# Print a decision tree\n",
    "def print_tree(node, depth=0):\n",
    "    if isinstance(node, dict):\n",
    "        print('depth:%s %s[X%d < %.3f]' % ((depth,depth*' ', (node['index']+1), node['value'])))\n",
    "        print_tree(node['left'], depth+1)\n",
    "        print_tree(node['right'], depth+1)\n",
    "    else:\n",
    "        print('depth:%s %s[%s]' % ((depth,depth*' ',str(node)+'_Leaf')))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3b0e7122-75ae-40eb-85bc-615ab7f55602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left : 1\n",
      "right : 1\n",
      "depth:0 [X1 < 6.642]\n",
      "depth:1  [0_Leaf]\n",
      "depth:1  [1_Leaf]\n"
     ]
    }
   ],
   "source": [
    "dataset = [[2.771244718,1.784783929,0],\n",
    "[1.728571309,1.169761413,0],\n",
    "[3.678319846,2.81281357,0],\n",
    "[3.961043357,2.61995032,0],\n",
    "[3.961043327,2.61995032,0],\n",
    "[3.961043357,2.61995032,0],\n",
    "[2.999208922,2.209014212,0],\n",
    "[7.497545867,3.162953546,1],\n",
    "[9.00220326,3.339047188,1],\n",
    "[7.444542326,0.476683375,1],\n",
    "[10.12493903,3.234550982,1],\n",
    "[6.642287351,3.319983761,1]]\n",
    "tree = build_tree(dataset, 3, 1)\n",
    "print_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cae7882d-0c93-424e-a5dd-b6b6e1480ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with a decision tree\n",
    "def predict(node, row):\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a6a071be-d79a-4123-9e51-d5e3a136a6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification and Regression Tree Algorithm\n",
    "def decision_tree(train, test, max_depth, min_size):\n",
    "    tree = build_tree(train, max_depth, min_size)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        prediction = predict(tree, row)\n",
    "        predictions.append(prediction)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0087e085-a9a9-4b00-a08e-e2028d95842f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file /Users/maheshwars/Desktop/venv/data/banknotes_data.csv with 1370 rows and 5 columns\n",
      "Scores: [96.35036496350365, 95.25547445255475, 96.71532846715328, 97.8102189781022, 97.44525547445255]\n",
      "Mean Accuracy: 96.715%\n"
     ]
    }
   ],
   "source": [
    "# Test CART on Bank Note dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filepath = '/Users/maheshwars/Desktop/venv/data'\n",
    "filename = filepath +'/banknotes_data.csv'\n",
    "dataset = load_csv(filename)\n",
    "print('Loaded data file {0} with {1} rows and {2} columns'.format(filename, len(dataset),\n",
    "                                                                  len(dataset[0])))\n",
    "\n",
    "# convert string attributes to integers\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "    \n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "max_depth = 5\n",
    "min_size = 10\n",
    "scores = evaluate_algorithm(dataset, decision_tree, n_folds, max_depth, min_size)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85a7759-ec4c-49f9-9f37-f4bc7016c0e8",
   "metadata": {},
   "source": [
    "# __Naive Bayes Classifier__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff449206-f2fe-478e-8319-065a3a7e949a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5.1783333865, 2.7665845055177263, 10), (2.9984683241, 1.218556343617447, 10)]\n"
     ]
    }
   ],
   "source": [
    "# Example of summarizing a dataset\n",
    "from math import sqrt\n",
    "\n",
    "# Calculate the mean of a list of numbers\n",
    "def mean(numbers):\n",
    "    return sum(numbers)/float(len(numbers))\n",
    "    \n",
    "# Calculate the standard deviation of a list of numbers\n",
    "def stdev(numbers):\n",
    "    avg = mean(numbers)\n",
    "    variance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1)\n",
    "    return sqrt(variance)\n",
    "\n",
    "# Calculate the mean, stdev and count for each column in a dataset\n",
    "def summarize_dataset(dataset):\n",
    "    summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]\n",
    "    del(summaries[-1])\n",
    "    return summaries\n",
    "\n",
    "# Test summarizing a dataset\n",
    "dataset = [[3.393533211,2.331273381,0],\n",
    "[3.110073483,1.781539638,0],\n",
    "[1.343808831,3.368360954,0],\n",
    "[3.582294042,4.67917911,0],\n",
    "[2.280362439,2.866990263,0],\n",
    "[7.423436942,4.696522875,1],\n",
    "[5.745051997,3.533989803,1],\n",
    "[9.172168622,2.511101045,1],\n",
    "[7.792783481,3.424088941,1],\n",
    "[7.939820817,0.791637231,1]]\n",
    "\n",
    "\n",
    "summary = summarize_dataset(dataset)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7b7f5de-a5b9-4e8f-8664-3836520f9bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3.393533211, 3.110073483, 1.343808831, 3.582294042, 2.280362439, 7.423436942, 5.745051997, 9.172168622, 7.792783481, 7.939820817)\n",
      "(2.331273381, 1.781539638, 3.368360954, 4.67917911, 2.866990263, 4.696522875, 3.533989803, 2.511101045, 3.424088941, 0.791637231)\n",
      "(0, 0, 0, 0, 0, 1, 1, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "for column in zip(*dataset): #dataset creates a list of all rows , and zip groupbys the rows by index, similar to what transpose does to a table\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1ab4067-83d9-4b49-a90a-67d26426fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset by class values, returns a dictionary\n",
    "def separate_by_class(dataset):\n",
    "    separated = dict()\n",
    "    for i in range(len(dataset)):\n",
    "        vector = dataset[i]\n",
    "        class_value = vector[-1]\n",
    "        if (class_value not in separated):\n",
    "            separated[class_value] = list()\n",
    "        separated[class_value].append(vector)\n",
    "    return separated\n",
    "\n",
    "def summarize_by_class(dataset):\n",
    "    separated = separate_by_class(dataset)\n",
    "    separated_summaries = dict()\n",
    "    for class_value, rows in separated.items():\n",
    "        separated_summaries[class_value] = summarize_dataset(rows)\n",
    "    return separated_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4653d1-a263-41f0-b74a-c649a84a732f",
   "metadata": {},
   "source": [
    "## Gaussian Probability density function\n",
    "\n",
    "Calculating the probability or likelihood of observing a given real-value like X1 is diﬃcult. One\n",
    "way we can do this is to assume that X1 values are drawn from a distribution, such as a bell\n",
    "curve or Gaussian distribution.\n",
    "A Gaussian distribution can be summarized using only two numbers: the mean and the\n",
    "standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f00ce712-0db6-4b05-bee1-ab472c2f7bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Gaussian probability distribution function for x\n",
    "def calculate_probability(x, mean, stdev):\n",
    "    exponent = exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
    "    return (1 / (sqrt(2 * pi) * stdev)) * exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2808dd39-f392-4b39-89ee-af8718c26a52",
   "metadata": {},
   "source": [
    "### Class Probabilities\n",
    "\n",
    "Now it is time to use the statistics calculated from our training data to calculate probabilities\n",
    "for new data. Probabilities are calculated separately for each class. This means that we first\n",
    "calculate the probability that a new piece of data belongs to the first class, then calculate\n",
    "probabilities that it belongs to the second class, and so on for all the classes. The probability\n",
    "that a piece of data belongs to a class is calculated as follows:\n",
    "\n",
    "P (class|data) = P (X|class) ×P (class) \n",
    "\n",
    "\n",
    "You may note that this is diﬀerent from the Bayes Theorem described above. The division\n",
    "have been removed to simplify the calculation. This means that the result is no longer strictly a\n",
    "probability of the data belonging to a class. The value is still maximized, meaning that the\n",
    "calculation for the class that results in the largest value is taken as the prediction. This is a\n",
    "common implementation simplification as we are often more interested in the class prediction\n",
    "rather than the probability.\n",
    "\n",
    "P (class = 0|X1, X2) = P (X1|class = 0) ×P (X2|class = 0) ×P (class = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1ac8370-dd75-48e4-9873-d49df7bf936c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.05032427673372076, 1: 0.00011557718379945765}\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from math import pi\n",
    "from math import exp\n",
    "\n",
    "def calculate_class_probabilities_ms(summaries,data):\n",
    "    p_class = []\n",
    "    for d_class,d_stats in summaries.items():\n",
    "        prob = d_stats[0][2]/sum([stat[0][2] for key,stat in summaries.items()]) #inefficient\n",
    "        for i in range(len(data)-1):\n",
    "            prob *= calculate_probability(data[i],d_stats[i][0],d_stats[i][1])\n",
    "        p_class.append(prob)\n",
    "    return p_class    \n",
    "\n",
    "# Calculate the probabilities of predicting each class for a given row\n",
    "def calculate_class_probabilities(summaries, row):\n",
    "    total_rows = sum([summaries[label][0][2] for label in summaries])\n",
    "    probabilities = dict()\n",
    "    for class_value, class_summaries in summaries.items():\n",
    "        probabilities[class_value] = summaries[class_value][0][2]/float(total_rows)\n",
    "        for i in range(len(class_summaries)):\n",
    "            mean, stdev, _ = class_summaries[i]\n",
    "            probabilities[class_value] *= calculate_probability(row[i], mean, stdev)\n",
    "    return probabilities\n",
    "    \n",
    "# Predict the class for a given row\n",
    "def predict(summaries, row):\n",
    "    probabilities = calculate_class_probabilities(summaries, row)\n",
    "    best_label, best_prob = None, -1\n",
    "    for class_value, probability in probabilities.items():\n",
    "        if best_label is None or probability > best_prob:\n",
    "            best_prob = probability\n",
    "            best_label = class_value\n",
    "    return best_label\n",
    "\n",
    "# Naive Bayes Algorithm\n",
    "def naive_bayes(train, test):\n",
    "    summarize = summarize_by_class(train)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        output = predict(summarize, row)\n",
    "        predictions.append(output)\n",
    "    return(predictions)\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores    \n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "        \n",
    "# Test calculating class probabilities\n",
    "dataset = [[3.393533211,2.331273381,0],\n",
    "[3.110073483,1.781539638,0],\n",
    "[1.343808831,3.368360954,0],\n",
    "[3.582294042,4.67917911,0],\n",
    "[2.280362439,2.866990263,0],\n",
    "[7.423436942,4.696522875,1],\n",
    "[5.745051997,3.533989803,1],\n",
    "[9.172168622,2.511101045,1],\n",
    "[7.792783481,3.424088941,1],\n",
    "[7.939820817,0.791637231,1]]\n",
    "\n",
    "summaries = summarize_by_class(dataset)\n",
    "probabilities = calculate_class_probabilities(summaries, dataset[0])\n",
    "print(probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "504cbd69-3c52-4256-b910-52d0008bdd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file /Users/maheshwars/Desktop/venv/data/iris/iris_data.csv with 150 rows and 5 columns\n",
      "Scores: [93.33333333333333, 96.66666666666667, 100.0, 93.33333333333333, 93.33333333333333]\n",
      "Mean Accuracy: 95.333%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filepath = '/Users/maheshwars/Desktop/venv/data/iris'\n",
    "filename = filepath +'/iris_data.csv'\n",
    "dataset = load_csv(filename)\n",
    "print('Loaded data file {0} with {1} rows and {2} columns'.format(filename, len(dataset),\n",
    "                                                                  len(dataset[0])))\n",
    "\n",
    "for i in range(len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "    \n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "scores = evaluate_algorithm(dataset, naive_bayes, n_folds)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bfd735-3513-4868-9ec5-4f6f50197091",
   "metadata": {},
   "source": [
    "# __K_Nearest Neighbours__\n",
    "\n",
    "The k-Nearest Neighbors algorithm or KNN for short is a very simple technique. The entire\n",
    "training dataset is stored. When a prediction is required, the k-most similar records to a\n",
    "new record from the training dataset are then located. From these neighbors, a summarized\n",
    "prediction is made. Similarity between records can be measured many diﬀerent ways. A problem\n",
    "or data-specific method can be used. Generally, with tabular data, a good starting point is the\n",
    "Euclidean distance.\n",
    "Once the neighbors are discovered, the summary prediction can be made by returning the\n",
    "most common outcome or taking the average. As such, KNN can be used for classification\n",
    "or regression problems. There is no model to speak of other than holding the entire training\n",
    "dataset. Because no work is done until a prediction is required, KNN is often referred to as a\n",
    "lazy learning method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c46fa69b-2034-4c9c-bdbf-2da88f027b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-nearest neighbors on the Abalone Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "    \n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        value_min = min(col_values)\n",
    "        value_max = max(col_values)\n",
    "        minmax.append([value_min, value_max])\n",
    "    return minmax\n",
    "\n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)-1):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for _ in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "\n",
    "    \n",
    "# Calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "    distance = 0.0\n",
    "    for i in range(len(row1)-1):\n",
    "        distance += (row1[i] - row2[i])**2\n",
    "    return sqrt(distance)\n",
    "    \n",
    "# Locate the most similar neighbors\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "    distances = list()\n",
    "    for train_row in train:\n",
    "        dist = euclidean_distance(test_row, train_row)\n",
    "        distances.append((train_row, dist))\n",
    "    distances.sort(key=lambda tup: tup[1])\n",
    "    neighbors = list()\n",
    "    for i in range(num_neighbors):\n",
    "        neighbors.append(distances[i][0])\n",
    "    return neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76466137-55d4-4d82-b24c-26a4088586d1",
   "metadata": {},
   "source": [
    "### Abalone Case Study as Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773761dc-767a-4c2d-b502-c9f38473048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "    \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted) #accuracy for classification\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "# Make a prediction with neighbors\n",
    "def predict_classification(train, test_row, num_neighbors):\n",
    "    neighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "    output_values = [row[-1] for row in neighbors]\n",
    "    prediction = max(set(output_values), key=output_values.count)\n",
    "    return prediction\n",
    "\n",
    "# kNN Algorithm\n",
    "def k_nearest_neighbors(train, test, num_neighbors):\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        output = predict_classification(train, row, num_neighbors)\n",
    "        predictions.append(output)\n",
    "    return (predictions)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e7d090f-26ed-420f-9ff9-b624d929039c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file /Users/maheshwars/Desktop/venv/data/abalone/abalone_data.csv with 4177 rows and 9 columns\n",
      "Scores: [24.790419161676645, 21.79640718562874, 23.592814371257482, 21.676646706586826, 23.353293413173652]\n",
      "Mean Accuracy: 23.042%\n"
     ]
    }
   ],
   "source": [
    "# Test the kNN on the Abalone dataset\n",
    "\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filepath = '/Users/maheshwars/Desktop/venv/data/abalone'\n",
    "filename = filepath +'/abalone_data.csv'\n",
    "dataset = load_csv(filename)\n",
    "print('Loaded data file {0} with {1} rows and {2} columns'.format(filename, len(dataset),\n",
    "                                                                  len(dataset[0])))\n",
    "\n",
    "for i in range(1, len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "\n",
    "# convert first column to integers\n",
    "str_column_to_int(dataset, 0)\n",
    "\n",
    "\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "num_neighbors = 5\n",
    "scores = evaluate_algorithm(dataset, k_nearest_neighbors, n_folds, num_neighbors)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8b2055-f061-4554-9b53-6a905491ae4c",
   "metadata": {},
   "source": [
    "### Abalone Case Study as Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f70fd390-fd8d-4217-bf8e-91568a8733a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate root mean squared error\n",
    "def rmse_metric(actual, predicted):\n",
    "    sum_error = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        prediction_error = predicted[i] - actual[i]\n",
    "        sum_error += (prediction_error ** 2)\n",
    "    mean_error = sum_error / float(len(actual))\n",
    "    return sqrt(mean_error)\n",
    "    \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        rmse = rmse_metric(actual, predicted) # rmse for regression\n",
    "        scores.append(rmse)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# Make a prediction with neighbors\n",
    "def predict_regression(train, test_row, num_neighbors):\n",
    "    neighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "    output_values = [row[-1] for row in neighbors]\n",
    "    prediction = sum(output_values) / float(len(output_values))\n",
    "    return prediction\n",
    "    \n",
    "# kNN Algorithm\n",
    "def k_nearest_neighbors(train, test, num_neighbors):\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        output = predict_regression(train, row, num_neighbors)\n",
    "        predictions.append(output)\n",
    "    return(predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "808cca71-c42a-49bb-8327-74316ee0a538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file /Users/maheshwars/Desktop/venv/data/abalone/abalone_data.csv with 4177 rows and 9 columns\n",
      "Scores: [2.170383116929243, 2.2087035241256405, 2.2321118594939215, 2.4013070293283603, 2.2274928845898017]\n",
      "Mean RMSE: 2.248\n"
     ]
    }
   ],
   "source": [
    "# Test the kNN on the Abalone dataset\n",
    "\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filepath = '/Users/maheshwars/Desktop/venv/data/abalone'\n",
    "filename = filepath +'/abalone_data.csv'\n",
    "dataset = load_csv(filename)\n",
    "print('Loaded data file {0} with {1} rows and {2} columns'.format(filename, len(dataset),\n",
    "                                                                  len(dataset[0])))\n",
    "\n",
    "for i in range(1, len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "\n",
    "# convert first column to integers\n",
    "str_column_to_int(dataset, 0)\n",
    "\n",
    "\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "num_neighbors = 5\n",
    "scores = evaluate_algorithm(dataset, k_nearest_neighbors, n_folds, num_neighbors)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean RMSE: %.3f' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada2ec20-94e6-4cb1-a97e-d9c1b54151c4",
   "metadata": {},
   "source": [
    "# __Learning Vector Quantization__\n",
    "\n",
    "A limitation of k-Nearest Neighbors is that you must keep a large database of training examples\n",
    "in order to make predictions. The Learning Vector Quantization algorithm addresses this by\n",
    "learning a much smaller subset of patterns that best represent the training data.\n",
    "\n",
    "The Learning Vector Quantization (LVQ) algorithm is a lot like k-Nearest Neighbors. __Predictions\n",
    "are made by finding the best match among a library of patterns__. The diﬀerence is that the library\n",
    "of patterns is learned from training data, rather than using the training patterns themselves.\n",
    "\n",
    "The library of patterns is called __codebook vectors__ and each pattern is called a codebook.\n",
    "The codebook vectors are initialized to randomly selected values from the training dataset.\n",
    "Then, over a number of epochs, __they are adapted to best summarize the training data using a\n",
    "learning algorithm. The learning algorithm shows one training record at a time, finds the best\n",
    "matching unit among the codebook vectors and moves it closer to the training record if they\n",
    "have the same class, or further away if they have diﬀerent classes.__\n",
    "\n",
    "Once prepared, the codebook vectors are used to make predictions using the k-Nearest\n",
    "Neighbors algorithm where **k=1**. The algorithm was developed for classification predictive\n",
    "modeling problems, but can be adapted for use with regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667fb404-ce39-453a-ad26-ebba1dc6c2b4",
   "metadata": {},
   "source": [
    "### Best Matching Unit\n",
    "The Best Matching Unit or BMU is the codebook vector that is most similar to a new piece of\n",
    "data. To locate the BMU for a new piece of data within a dataset we must first calculate the\n",
    "distance between each codebook to the new piece of data. We can do this using our distance\n",
    "function above. Once distances are calculated, we must sort all of the codebooks by their\n",
    "distance to the new data. We can then return the first or most similar codebook vector.\n",
    "\n",
    "We can do this by keeping track of the distance for each record in the dataset as a tuple,\n",
    "sort the list of tuples by the distance (in descending order) and then retrieve the BMU. Below\n",
    "is a function named get best matching unit() that implements this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "367bb641-36a5-4540-aba5-a23141e0ae94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.7810836, 2.550537003, 0]\n"
     ]
    }
   ],
   "source": [
    "# Example of getting the BMU\n",
    "from math import sqrt\n",
    "\n",
    "# calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "    distance = 0.0\n",
    "    for i in range(len(row1)-1):\n",
    "        distance += (row1[i] - row2[i])**2\n",
    "    return sqrt(distance)\n",
    "\n",
    "# Locate the best matching unit\n",
    "def get_best_matching_unit(codebooks, test_row):\n",
    "    distances = list()\n",
    "    for codebook in codebooks:\n",
    "        dist = euclidean_distance(codebook, test_row)\n",
    "        distances.append((codebook, dist))\n",
    "    distances.sort(key=lambda tup: tup[1])\n",
    "    return distances[0][0]\n",
    "\n",
    "# Test best matching unit function\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "[1.465489372,2.362125076,0],\n",
    "[3.396561688,4.400293529,0],\n",
    "[1.38807019,1.850220317,0],\n",
    "[3.06407232,3.005305973,0],\n",
    "[7.627531214,2.759262235,1],\n",
    "[5.332441248,2.088626775,1],\n",
    "[6.922596716,1.77106367,1],\n",
    "[8.675418651,-0.242068655,1],\n",
    "[7.673756466,3.508563011,1]]\n",
    "test_row = dataset[0]\n",
    "bmu = get_best_matching_unit(dataset, test_row)\n",
    "print(bmu)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d197661d-4358-488d-8c74-853b7af1659a",
   "metadata": {},
   "source": [
    "### Training Codebook Vectors\n",
    "The first step in training a set of codebook vectors is to initialize the set. We can initialize it with\n",
    "patterns constructed from random features in the training dataset. Below is a function named\n",
    "random codebook() that implements this. Random input and output features are selected from\n",
    "the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4d092eb-8cc7-4879-9e4b-fb77b1ca5676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random codebook vector\n",
    "def random_codebook(train):\n",
    "    n_records = len(train)\n",
    "    n_features = len(train[0])\n",
    "    codebook = [train[randrange(n_records)][i] for i in range(n_features)]\n",
    "    return codebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1792ed1-307a-4a89-b8cd-1a4a2315ddaa",
   "metadata": {},
   "source": [
    "The best matching unit is found for each training pattern and only this best matching unit\n",
    "is updated. The diﬀerence between the training pattern and the BMU is calculated as the error.\n",
    "The class values (assumed to be the last value in the list) are compared. If they match, the\n",
    "error is added to the BMU to bring it closer to the training pattern, otherwise it is subtracted\n",
    "to push it further away.\n",
    "\n",
    "The amount that the BMU is adjusted is controlled by a __learning rate__. This is a weighting\n",
    "on the amount of change made to all BMUs. For example, a learning rate of 0.3 means that\n",
    "BMUs are only moved by 30% of the error or diﬀerence between training patterns and BMUs.\n",
    "Further, the learning rate is adjusted so that it has maximum eﬀect in the first epoch and\n",
    "less eﬀect as training continues until it has a minimal eﬀect in the final epoch. This is called a\n",
    "**linear decay learning rate schedule** and can also be used in artificial neural networks. We can\n",
    "summarize this decay in learning rate by epoch number as follows:\n",
    "\n",
    "**rate = learning rate × (1.0 − (epoch / total_epochs))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc60d9a1-c51a-40cf-bcd7-e71f7dda0a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a set of codebook vectors\n",
    "def train_codebooks(train, n_codebooks, lrate, epochs):\n",
    "    codebooks = [random_codebook(train) for i in range(n_codebooks)]\n",
    "    for epoch in range(epochs):\n",
    "        rate = lrate * (1.0-(epoch/float(epochs)))\n",
    "        sum_error = 0.0\n",
    "        for row in train:\n",
    "            bmu = get_best_matching_unit(codebooks, row)\n",
    "            for i in range(len(row)-1):\n",
    "                error = row[i] - bmu[i]\n",
    "                sum_error += error**2\n",
    "                if bmu[-1] == row[-1]:\n",
    "                    bmu[i] += rate * error\n",
    "                else:\n",
    "                    bmu[i] -= rate * error\n",
    "        #print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, rate, sum_error))\n",
    "    return codebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c589c7cd-6fbd-4db5-b6a5-d60523804afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e62556c-217f-4e42-88bc-62d09c08642d",
   "metadata": {},
   "source": [
    "### Training and prediction on ionosphere dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "36da2773-1e65-44e8-825c-3a9695b36324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file /Users/maheshwars/Desktop/venv/data/ionosphere/ionosphere_data.csv with 351 rows and 35 columns\n",
      "Lrate :0.3, learning rate Scores: [88.57142857142857, 90.0, 88.57142857142857, 88.57142857142857, 80.0]\n",
      "Mean Accuracy: 87.143%\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction with codebook vectors\n",
    "def predict(codebooks, test_row):\n",
    "    bmu = get_best_matching_unit(codebooks, test_row)\n",
    "    return bmu[-1]\n",
    "\n",
    "# LVQ Algorithm\n",
    "def learning_vector_quantization(train, test, n_codebooks, lrate, epochs):\n",
    "    codebooks = train_codebooks(train, n_codebooks, lrate, epochs)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        output = predict(codebooks, row)\n",
    "        predictions.append(output)\n",
    "    return(predictions)\n",
    "    \n",
    "# Test LVQ on Ionosphere dataset\n",
    "seed(1)\n",
    "\n",
    "# load and prepare data\n",
    "filepath = '/Users/maheshwars/Desktop/venv/data/ionosphere'\n",
    "filename = filepath +'/ionosphere_data.csv'\n",
    "dataset = load_csv(filename)\n",
    "print('Loaded data file {0} with {1} rows and {2} columns'.format(filename, len(dataset),\n",
    "                                                                  len(dataset[0])))\n",
    "\n",
    "for i in range(len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "# evaluate algorithm\n",
    "# for i in range(1,15,2):\n",
    "n_folds = 5\n",
    "learn_rate = 0.3\n",
    "n_epochs = 50\n",
    "n_codebooks = 20\n",
    "scores = evaluate_algorithm(dataset, learning_vector_quantization, n_folds, n_codebooks,\n",
    "learn_rate, n_epochs)\n",
    "print('Lrate :{1}, learning rate Scores: {0}'.format(scores,learn_rate))\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b81179-ac62-4160-9d7e-ccbe8d60f200",
   "metadata": {},
   "source": [
    "# __Backpropagation__\n",
    "\n",
    "The principle of the backpropagation approach is to model a given function by modifying\n",
    "internal weightings of input signals to produce an expected output signal. The system is trained\n",
    "using a supervised learning method where the error between the system’s output and a known\n",
    "expected output is presented to the system and used to modify its internal state.\n",
    "\n",
    "Technically, the backpropagation algorithm is a method for training the weights in a multilayer\n",
    "feedforward neural network. As such, it requires a network structure to be defined of one or\n",
    "more layers where one layer is fully connected to the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e54e2fd-fdf3-4931-9094-1609e9f49f3b",
   "metadata": {},
   "source": [
    "## Initialize Network\n",
    "Let’s start with something easy: the creation of a new network ready for training. Each neuron\n",
    "has a set of weights that need to be maintained. One weight for each input connection and an\n",
    "additional weight for the bias. We will need to store additional properties for a neuron during\n",
    "training, therefore **we will use a dictionary to represent each neuron and store properties by\n",
    "names such as weights for the weights**.\n",
    "A network is organized into layers. The input layer is really just a row from our training\n",
    "dataset. The first real layer is the hidden layer. This is followed by the output layer that has\n",
    "one neuron for each class value. **We will organize layers as arrays of dictionaries and treat the\n",
    "whole network as an array of layers**. It is good practice to initialize the network weights to\n",
    "small random numbers. In this case, will we use random numbers in the range of 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db13bc9-38eb-4110-bcc3-e58b622ea659",
   "metadata": {},
   "source": [
    "Below is a function named initialize network() that creates a new neural network ready\n",
    "for training. It accepts three parameters: the number of inputs, the number of neurons to have\n",
    "in the hidden layer and the number of outputs. You can see that for the hidden layer we create\n",
    "n hidden neurons and **each neuron** in the hidden layer has **n inputs + 1 weights**, one for each\n",
    "input column in a dataset and an additional one for the bias.\n",
    "\n",
    "You can also see that the output layer that connects to the hidden layer has n outputs\n",
    "neurons, **each neuron with n hidden + 1 weights**. This means that each neuron in the output layer\n",
    "connects to (has a weight for) each neuron in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "647e7920-9dc9-443d-82d1-7e3f9f722fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in\n",
    "    range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in\n",
    "    range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11168ff-87bd-4472-b6ee-71fd39516637",
   "metadata": {},
   "source": [
    "## Forward-Propagate\n",
    "\n",
    "We can break forward-propagation down into three parts:\n",
    "\n",
    "1. __Neuron Activation:__\n",
    "\n",
    "   \n",
    "    Neuron activation is calculated as the weighted sum of the inputs. Much like linear regression.\n",
    "\n",
    "\n",
    "         activation = bias + summation{ weights[i] * inputs[i] }\n",
    "\n",
    "      \n",
    "3. __Neuron Transfer:__\n",
    "\n",
    "   \n",
    "    Once a neuron is activated, we need to transfer the activation to see what the neuron output\n",
    "    actually is. Diﬀerent transfer functions can be used. It is traditional to use the sigmoid activation\n",
    "    function, but you can also use the tanh (hyperbolic tangent) function to transfer outputs. More\n",
    "    recently, the rectifier transfer function has been popular with large deep learning networks.\n",
    "   \n",
    "5. __Forward-Propagation:__\n",
    "    \n",
    "    We work through each layer of our network\n",
    "    calculating the outputs for each neuron. All of the outputs from one layer become inputs to the\n",
    "    neurons on the next layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "87618c93-f7ee-438a-9f90-dfe20de7c9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1] #bias column\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    "\n",
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + exp(-activation))    \n",
    "\n",
    "# Forward-propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc184cb4-fbbe-45a1-8266-63432ca61f22",
   "metadata": {},
   "source": [
    "## Backpropagate Error\n",
    "The backpropagation algorithm is named for the way in which weights are trained. Error is\n",
    "calculated between the expected outputs and the outputs forward-propagated from the network.\n",
    "These **errors are then propagated backward** through the network from the output layer to the\n",
    "hidden layer, **assigning blame for the error** and updating weights as they go. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb7fb29-59b4-4164-9e06-ed12735e5948",
   "metadata": {},
   "source": [
    "### Transfer Derivative\n",
    "Given an output value from a neuron, we need to calculate it’s slope. We are using the sigmoid\n",
    "transfer function, the derivative of which can be calculated as follows:\n",
    "\n",
    "    derivative= output ×(1.0−output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "064079e6-1e57-49d3-b9b6-a43901b9db0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495f09ad-7bc4-4cce-bdf1-2bd2bd0d5bc5",
   "metadata": {},
   "source": [
    "### Error Backpropagation\n",
    "\n",
    "The first step is to calculate the __error for each output neuron__; this will give us our __error signal\n",
    "(input) to propagate backwards through the network__. The error for a given neuron can be\n",
    "calculated as follows:\n",
    "\n",
    "    error = (expected−output) ×transfer derivative(output)\n",
    "\n",
    "\n",
    "Where expected is the expected output value for the neuron, output is the output value\n",
    "for the neuron and transfer_derivative() calculates the slope of the neuron’s output value,\n",
    "as shown above. This error calculation is used for neurons in the output layer. The expected\n",
    "value is the class value itself. In the hidden layer, things are a little more complicated.\n",
    "\n",
    "\n",
    "The error signal for a neuron in the hidden layer is calculated as the weighted error of each\n",
    "neuron in the output layer. Think of the error traveling back along the weights of the output\n",
    "layer to the neurons in the hidden layer. The backpropagated error signal is accumulated and\n",
    "then used to determine the error for the neuron in the hidden layer, as follows:\n",
    "\n",
    "    error = (weightk ×errorj ) ×transfer derivative(output) \n",
    "\n",
    "\n",
    "Where error j is the error signal from the jth neuron in the output layer, weight k is the\n",
    "weight that connects the kth neuron to the current neuron(neuron of hidden layer) and output is the output for the\n",
    "current neuron. Below is a function named backward propagate error() that implements this\n",
    "procedure.\n",
    "\n",
    "You can see that the error signal calculated for each neuron is stored with the name delta.\n",
    "You can see that the layers of the network are iterated in reverse order, starting at the output\n",
    "and working backwards. This ensures that the neurons in the output layer have delta values\n",
    "calculated first that neurons in the hidden layer can use in the subsequent iteration.\n",
    "You can see that **the error signal for neurons in the hidden layer is accumulated from neurons\n",
    "in the output layer where the hidden neuron number j is also the index of the neuron’s weight\n",
    "in the output layer neuron[’weights’][j].**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "00e45b6a-0a01-4d17-85d7-7b987c271c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "125895b3-8bdd-4980-8597-a599c383323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e6256-0f8b-4f3f-b0c0-fe657c1c8116",
   "metadata": {},
   "source": [
    "### Train Network\n",
    "As mentioned, the network is updated using stochastic gradient descent. This involves first\n",
    "looping for a fixed number of epochs and within each epoch updating the network for each row\n",
    "in the training dataset.\n",
    "\n",
    "Because updates are made for each training pattern, this type of learning is called online\n",
    "learning. If errors were accumulated across an epoch before updating the weights, this is called\n",
    "batch learning or batch gradient descent. Below is a function that implements the training of\n",
    "an already initialized neural network with a given training dataset, learning rate, fixed number\n",
    "of epochs and an expected number of output values.\n",
    "\n",
    "The expected number of output values is used to transform class values in the training data\n",
    "into a one hot encoding. That is a binary vector with one column for each class value to match\n",
    "the output of the network. This is required to calculate the error for the output layer. You can\n",
    "also see that the sum squared error between the expected output and the network output is\n",
    "accumulated each epoch and printed. This is helpful to create a trace of how much the network\n",
    "is learning and improving each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f0ed7d60-b3d8-4b0b-8277-b11f7d100104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "        #print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dc019fc6-0198-45b5-a0c7-ec8c1ee3ef1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.500, error=6.371\n",
      ">epoch=1, lrate=0.500, error=5.584\n",
      ">epoch=2, lrate=0.500, error=5.331\n",
      ">epoch=3, lrate=0.500, error=5.314\n",
      ">epoch=4, lrate=0.500, error=5.330\n",
      ">epoch=5, lrate=0.500, error=5.341\n",
      ">epoch=6, lrate=0.500, error=5.346\n",
      ">epoch=7, lrate=0.500, error=5.347\n",
      ">epoch=8, lrate=0.500, error=5.347\n",
      ">epoch=9, lrate=0.500, error=5.346\n",
      ">epoch=10, lrate=0.500, error=5.345\n",
      ">epoch=11, lrate=0.500, error=5.343\n",
      ">epoch=12, lrate=0.500, error=5.342\n",
      ">epoch=13, lrate=0.500, error=5.340\n",
      ">epoch=14, lrate=0.500, error=5.339\n",
      ">epoch=15, lrate=0.500, error=5.337\n",
      ">epoch=16, lrate=0.500, error=5.335\n",
      ">epoch=17, lrate=0.500, error=5.334\n",
      ">epoch=18, lrate=0.500, error=5.332\n",
      ">epoch=19, lrate=0.500, error=5.330\n",
      "[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614], 'output': 0.99157530623528, 'delta': -0.0005341455083516953}, {'weights': [0.2550690257394217, 0.49543508709194095, 0.4494910647887381], 'output': 0.9844051047537846, 'delta': 0.0011954440011118744}]\n",
      "[{'weights': [0.1352957155754806, -0.06993695351348819, -0.5152093452497912], 'output': 0.42656544528564916, 'delta': -0.10434105007212165}, {'weights': [-0.38274067993191285, 0.7914341824323032, 0.07234040575369743], 'output': 0.5794832633351258, 'delta': 0.10247253219310253}]\n"
     ]
    }
   ],
   "source": [
    "# Example of training a network by backpropagation\n",
    "from math import exp\n",
    "from random import seed\n",
    "from random import random\n",
    "\n",
    "# Test training backprop algorithm\n",
    "seed(1)\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "[1.465489372,2.362125076,0],\n",
    "[3.396561688,4.400293529,0],\n",
    "[1.38807019,1.850220317,0],\n",
    "[3.06407232,3.005305973,0],\n",
    "[7.627531214,2.759262235,1],\n",
    "[5.332441248,2.088626775,1],\n",
    "[6.922596716,1.77106367,1],\n",
    "[8.675418651,-0.242068655,1],\n",
    "[7.673756466,3.508563011,1]]\n",
    "n_inputs = len(dataset[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in dataset]))\n",
    "network = initialize_network(n_inputs, 2, n_outputs)\n",
    "train_network(network, dataset, 0.5, 20, n_outputs)\n",
    "for layer in network:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5b8dc60b-8e8a-4877-bba7-d17cf2f2f2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))\n",
    "\n",
    "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
    "def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n",
    "    n_inputs = len(train[0]) - 1\n",
    "    n_outputs = len(set([row[-1] for row in train]))\n",
    "    network = initialize_network(n_inputs, n_hidden, n_outputs)\n",
    "    train_network(network, train, l_rate, n_epoch, n_outputs)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        prediction = predict(network, row)\n",
    "        predictions.append(prediction)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ca1b4bf5-3560-413a-8106-88587c5bf16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file /Users/maheshwars/Desktop/venv/data//seeds_data.txt with 210 rows and 8 columns\n",
      "['15.26', '14.84', '0.871', '5.763', '3.312', '2.221', '5.22', '1']\n",
      "[15.26, 14.84, 0.871, 5.763, 3.312, 2.221, 5.22, 2]\n",
      "[0.4409820585457979, 0.5020661157024793, 0.570780399274047, 0.48648648648648646, 0.48610121168923714, 0.18930164220052273, 0.3451501723289019, 2]\n"
     ]
    }
   ],
   "source": [
    "# Test Backprop on Seeds dataset\n",
    "seed(1)\n",
    "\n",
    "def load_txt(filename):\n",
    "    dataset = []\n",
    "    with open(filename, \"r\") as file:\n",
    "        for line in file:\n",
    "            # Split by spaces and convert to floats (or int, if applicable)\n",
    "            values = list(line.strip().split())\n",
    "            dataset.append(values)\n",
    "    return dataset\n",
    "\n",
    "# load and prepare data\n",
    "filepath = '/Users/maheshwars/Desktop/venv/data/'\n",
    "filename = filepath +'/seeds_data.txt'\n",
    "dataset = load_txt(filename)\n",
    "print('Loaded data file {0} with {1} rows and {2} columns'.format(filename, len(dataset),\n",
    "                                                                  len(dataset[0])))\n",
    "\n",
    "print(dataset[0])\n",
    "for i in range(len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "\n",
    "\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "# normalize input variables\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)\n",
    "\n",
    "\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dee14b06-2337-4cb0-a13d-7e1a1dc883e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [92.85714285714286, 95.23809523809523, 95.23809523809523, 95.23809523809523, 92.85714285714286]\n",
      "Mean Accuracy: 94.286%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "l_rate = 0.3\n",
    "n_epoch = 700\n",
    "n_hidden = 8\n",
    "\n",
    "scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a995994-015d-4627-bbf5-2688a5f40626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
